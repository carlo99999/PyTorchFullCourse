{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST,CIFAR10,FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor,ToPILImage\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import random\n",
    "import os\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set device agnostic code\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data=FashionMNIST(root=\"data\",train=True,download=True,transform=ToTensor())\n",
    "test_data=FashionMNIST(root=\"data\",train=False,download=True,transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '9')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualize the first image\n",
    "image,label=train_data[0]\n",
    "plt.imshow(image.squeeze(),cmap=\"gray\")\n",
    "plt.title(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Create dataloaders\n",
    "BATCH_SIZE=32\n",
    "train_dataloader=DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dataloader=DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel_CNN_V0(\n",
       "  (conv_layer_1): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_2): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_layer_3): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc_layer): Linear(in_features=270, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create first CNN model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FashionMNISTModel_CNN_V0(nn.Module):\n",
    "    def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1=nn.Conv2d(in_channels=input_shape,out_channels=hidden_units,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_layer_2=nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv_layer_3=nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc_layer=nn.Linear(in_features=hidden_units*3*3,out_features=output_shape)\n",
    "        \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        x=self.pool(F.relu(self.conv_layer_1(x)))\n",
    "        x=self.pool(F.relu(self.conv_layer_2(x)))\n",
    "        x=self.pool(F.relu(self.conv_layer_3(x)))\n",
    "        x=self.flatten(x)\n",
    "        x=self.fc_layer(x)\n",
    "        return x\n",
    "    \n",
    "model=FashionMNISTModel_CNN_V0(input_shape=1,hidden_units=30,output_shape=len(train_data.classes)).to(device)\n",
    "\n",
    "model.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19270 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "## Create a function to count the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"The model has {count_parameters(model)} trainable parameters\")\n",
    "\n",
    "## Create a function to calculate the accuracy\n",
    "def accuracy_fn(y_true,y_pred):\n",
    "    correct=torch.eq(y_true,y_pred).sum().item()\n",
    "    acc=correct/len(y_pred)*100\n",
    "    return acc\n",
    "\n",
    "## Create a loss function and optimizer\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(params=model.parameters(),lr=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a training loop\n",
    "def train_step(model:nn.Module,data_loader:DataLoader,loss_fn:nn.Module,optimizer:optim.Optimizer,device:torch.device):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    \n",
    "    for batch,(X,y) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        y_pred=model(X)\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        train_loss+=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred_class=torch.argmax(y_pred,dim=1)\n",
    "        train_acc+=accuracy_fn(y,y_pred_class)\n",
    "        \n",
    "    train_loss/=len(data_loader)\n",
    "    train_acc/=len(data_loader)\n",
    "    \n",
    "    return train_loss,train_acc\n",
    "\n",
    "\n",
    "def test_step(model:nn.Module,data_loader:DataLoader,loss_fn:nn.Module,device:torch.device):\n",
    "    \n",
    "    \n",
    "    test_loss,test_acc=0,0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X,y in data_loader:\n",
    "            X,y=X.to(device),y.to(device)\n",
    "            \n",
    "            y_pred=model(X)\n",
    "            loss=loss_fn(y_pred,y)\n",
    "            test_loss+=loss\n",
    "            \n",
    "            y_pred_class=torch.argmax(y_pred,dim=1)\n",
    "            test_acc+=accuracy_fn(y,y_pred_class)\n",
    "            \n",
    "    tmp_test_loss=test_loss/len(data_loader)\n",
    "    tmp_test_acc=test_acc/len(data_loader)\n",
    "    \n",
    "    return tmp_test_loss,tmp_test_acc\n",
    "\n",
    "## Create a function to train and test the model\n",
    "def train(model:nn.Module,train_dataloader:DataLoader,test_dataloader:DataLoader,loss_fn:nn.Module,optimizer:optim.Optimizer,device:torch.device,epochs:int):\n",
    "    results={}\n",
    "    results[\"train_loss\"]=[]\n",
    "    results[\"train_acc\"]=[]\n",
    "    results[\"test_loss\"]=[]\n",
    "    results[\"test_acc\"]=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss,train_acc=train_step(model=model,data_loader=train_dataloader,loss_fn=loss_fn,optimizer=optimizer,device=device)\n",
    "        \n",
    "        test_loss,test_acc=test_step(model=model,data_loader=test_dataloader,loss_fn=loss_fn,device=device)\n",
    "        \n",
    "        print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f}% | Test loss: {test_loss:.4f} | Test acc: {test_acc:.2f}%\")\n",
    "        \n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.5837 | Train acc: 78.75% | Test loss: 0.3809 | Test acc: 86.25%\n",
      "Epoch: 1 | Train loss: 0.3475 | Train acc: 87.25% | Test loss: 0.3594 | Test acc: 86.71%\n",
      "Epoch: 2 | Train loss: 0.3052 | Train acc: 88.80% | Test loss: 0.3101 | Test acc: 88.82%\n",
      "Epoch: 3 | Train loss: 0.2804 | Train acc: 89.67% | Test loss: 0.3092 | Test acc: 88.68%\n",
      "Epoch: 4 | Train loss: 0.2614 | Train acc: 90.48% | Test loss: 0.2946 | Test acc: 89.51%\n",
      "Epoch: 5 | Train loss: 0.2490 | Train acc: 90.92% | Test loss: 0.2961 | Test acc: 89.53%\n",
      "Epoch: 6 | Train loss: 0.2388 | Train acc: 91.28% | Test loss: 0.2952 | Test acc: 89.53%\n",
      "Epoch: 7 | Train loss: 0.2277 | Train acc: 91.73% | Test loss: 0.3099 | Test acc: 88.94%\n",
      "Epoch: 8 | Train loss: 0.2189 | Train acc: 92.02% | Test loss: 0.2820 | Test acc: 90.22%\n",
      "Epoch: 9 | Train loss: 0.2134 | Train acc: 92.23% | Test loss: 0.2826 | Test acc: 90.18%\n",
      "Epoch: 10 | Train loss: 0.2081 | Train acc: 92.49% | Test loss: 0.2932 | Test acc: 89.51%\n",
      "Epoch: 11 | Train loss: 0.2026 | Train acc: 92.56% | Test loss: 0.2857 | Test acc: 90.26%\n",
      "Epoch: 12 | Train loss: 0.1967 | Train acc: 92.92% | Test loss: 0.2815 | Test acc: 90.61%\n",
      "Epoch: 13 | Train loss: 0.1920 | Train acc: 93.03% | Test loss: 0.2802 | Test acc: 90.41%\n",
      "Epoch: 14 | Train loss: 0.1875 | Train acc: 93.18% | Test loss: 0.2793 | Test acc: 90.33%\n",
      "Epoch: 15 | Train loss: 0.1846 | Train acc: 93.21% | Test loss: 0.2767 | Test acc: 90.41%\n",
      "Epoch: 16 | Train loss: 0.1782 | Train acc: 93.49% | Test loss: 0.2927 | Test acc: 90.21%\n",
      "Epoch: 17 | Train loss: 0.1756 | Train acc: 93.56% | Test loss: 0.3021 | Test acc: 89.70%\n",
      "Epoch: 18 | Train loss: 0.1709 | Train acc: 93.78% | Test loss: 0.2811 | Test acc: 90.67%\n",
      "Epoch: 19 | Train loss: 0.1676 | Train acc: 93.83% | Test loss: 0.3093 | Test acc: 90.57%\n",
      "Epoch: 20 | Train loss: 0.1663 | Train acc: 93.89% | Test loss: 0.2915 | Test acc: 90.67%\n",
      "Epoch: 21 | Train loss: 0.1630 | Train acc: 94.02% | Test loss: 0.3092 | Test acc: 90.00%\n",
      "Epoch: 22 | Train loss: 0.1600 | Train acc: 94.05% | Test loss: 0.3092 | Test acc: 90.31%\n",
      "Epoch: 23 | Train loss: 0.1585 | Train acc: 94.06% | Test loss: 0.3087 | Test acc: 90.18%\n",
      "Epoch: 24 | Train loss: 0.1564 | Train acc: 94.28% | Test loss: 0.3362 | Test acc: 89.72%\n",
      "Epoch: 25 | Train loss: 0.1520 | Train acc: 94.34% | Test loss: 0.3341 | Test acc: 90.46%\n",
      "Epoch: 26 | Train loss: 0.1508 | Train acc: 94.44% | Test loss: 0.3253 | Test acc: 90.19%\n",
      "Epoch: 27 | Train loss: 0.1489 | Train acc: 94.50% | Test loss: 0.3367 | Test acc: 90.14%\n",
      "Epoch: 28 | Train loss: 0.1469 | Train acc: 94.64% | Test loss: 0.3465 | Test acc: 90.42%\n",
      "Epoch: 29 | Train loss: 0.1465 | Train acc: 94.61% | Test loss: 0.3363 | Test acc: 90.22%\n",
      "Epoch: 30 | Train loss: 0.1441 | Train acc: 94.69% | Test loss: 0.3534 | Test acc: 89.49%\n",
      "Epoch: 31 | Train loss: 0.1409 | Train acc: 94.78% | Test loss: 0.3449 | Test acc: 89.20%\n",
      "Epoch: 32 | Train loss: 0.1393 | Train acc: 94.85% | Test loss: 0.3473 | Test acc: 89.88%\n",
      "Epoch: 33 | Train loss: 0.1401 | Train acc: 94.81% | Test loss: 0.3511 | Test acc: 89.58%\n",
      "Epoch: 34 | Train loss: 0.1392 | Train acc: 94.91% | Test loss: 0.3446 | Test acc: 90.59%\n",
      "Epoch: 35 | Train loss: 0.1348 | Train acc: 94.98% | Test loss: 0.3689 | Test acc: 90.02%\n",
      "Epoch: 36 | Train loss: 0.1378 | Train acc: 95.03% | Test loss: 0.3781 | Test acc: 89.79%\n",
      "Epoch: 37 | Train loss: 0.1365 | Train acc: 94.99% | Test loss: 0.3578 | Test acc: 90.04%\n",
      "Epoch: 38 | Train loss: 0.1343 | Train acc: 94.97% | Test loss: 0.3936 | Test acc: 89.68%\n",
      "Epoch: 39 | Train loss: 0.1316 | Train acc: 95.23% | Test loss: 0.3629 | Test acc: 90.34%\n",
      "Epoch: 40 | Train loss: 0.1343 | Train acc: 95.07% | Test loss: 0.3932 | Test acc: 89.75%\n",
      "Epoch: 41 | Train loss: 0.1341 | Train acc: 95.06% | Test loss: 0.3782 | Test acc: 89.96%\n",
      "Epoch: 42 | Train loss: 0.1298 | Train acc: 95.16% | Test loss: 0.4185 | Test acc: 89.49%\n",
      "Epoch: 43 | Train loss: 0.1307 | Train acc: 95.18% | Test loss: 0.3960 | Test acc: 89.64%\n",
      "Epoch: 44 | Train loss: 0.1285 | Train acc: 95.22% | Test loss: 0.4445 | Test acc: 88.93%\n",
      "Epoch: 45 | Train loss: 0.1284 | Train acc: 95.25% | Test loss: 0.4249 | Test acc: 89.49%\n",
      "Epoch: 46 | Train loss: 0.1272 | Train acc: 95.39% | Test loss: 0.4478 | Test acc: 89.55%\n",
      "Epoch: 47 | Train loss: 0.1252 | Train acc: 95.42% | Test loss: 0.4232 | Test acc: 89.30%\n",
      "Epoch: 48 | Train loss: 0.1234 | Train acc: 95.47% | Test loss: 0.3819 | Test acc: 90.13%\n",
      "Epoch: 49 | Train loss: 0.1247 | Train acc: 95.51% | Test loss: 0.4149 | Test acc: 89.99%\n",
      "Epoch: 50 | Train loss: 0.1271 | Train acc: 95.33% | Test loss: 0.4022 | Test acc: 89.34%\n",
      "Epoch: 51 | Train loss: 0.1222 | Train acc: 95.55% | Test loss: 0.4448 | Test acc: 89.73%\n",
      "Epoch: 52 | Train loss: 0.1229 | Train acc: 95.50% | Test loss: 0.4479 | Test acc: 89.35%\n",
      "Epoch: 53 | Train loss: 0.1236 | Train acc: 95.51% | Test loss: 0.4324 | Test acc: 90.00%\n",
      "Epoch: 54 | Train loss: 0.1272 | Train acc: 95.41% | Test loss: 0.4743 | Test acc: 88.81%\n",
      "Epoch: 55 | Train loss: 0.1228 | Train acc: 95.54% | Test loss: 0.4693 | Test acc: 89.65%\n",
      "Epoch: 56 | Train loss: 0.1251 | Train acc: 95.53% | Test loss: 0.4155 | Test acc: 89.19%\n",
      "Epoch: 57 | Train loss: 0.1173 | Train acc: 95.75% | Test loss: 0.4260 | Test acc: 89.73%\n",
      "Epoch: 58 | Train loss: 0.1197 | Train acc: 95.73% | Test loss: 0.4562 | Test acc: 89.37%\n",
      "Epoch: 59 | Train loss: 0.1227 | Train acc: 95.64% | Test loss: 0.4442 | Test acc: 89.14%\n",
      "Epoch: 60 | Train loss: 0.1217 | Train acc: 95.60% | Test loss: 0.4777 | Test acc: 88.55%\n",
      "Epoch: 61 | Train loss: 0.1242 | Train acc: 95.47% | Test loss: 0.4345 | Test acc: 89.27%\n",
      "Epoch: 62 | Train loss: 0.1191 | Train acc: 95.70% | Test loss: 0.4628 | Test acc: 89.76%\n",
      "Epoch: 63 | Train loss: 0.1216 | Train acc: 95.64% | Test loss: 0.4764 | Test acc: 89.92%\n",
      "Epoch: 64 | Train loss: 0.1188 | Train acc: 95.72% | Test loss: 0.4924 | Test acc: 89.98%\n",
      "Epoch: 65 | Train loss: 0.1195 | Train acc: 95.72% | Test loss: 0.4504 | Test acc: 90.18%\n",
      "Epoch: 66 | Train loss: 0.1178 | Train acc: 95.80% | Test loss: 0.4629 | Test acc: 89.04%\n",
      "Epoch: 67 | Train loss: 0.1189 | Train acc: 95.72% | Test loss: 0.4846 | Test acc: 89.87%\n",
      "Epoch: 68 | Train loss: 0.1173 | Train acc: 95.81% | Test loss: 0.4868 | Test acc: 89.01%\n",
      "Epoch: 69 | Train loss: 0.1205 | Train acc: 95.70% | Test loss: 0.5247 | Test acc: 89.53%\n",
      "Epoch: 70 | Train loss: 0.1202 | Train acc: 95.73% | Test loss: 0.4938 | Test acc: 88.97%\n",
      "Epoch: 71 | Train loss: 0.1216 | Train acc: 95.64% | Test loss: 0.4831 | Test acc: 89.09%\n",
      "Epoch: 72 | Train loss: 0.1166 | Train acc: 95.70% | Test loss: 0.5011 | Test acc: 89.66%\n",
      "Epoch: 73 | Train loss: 0.1206 | Train acc: 95.81% | Test loss: 0.4732 | Test acc: 90.09%\n",
      "Epoch: 74 | Train loss: 0.1175 | Train acc: 95.80% | Test loss: 0.4772 | Test acc: 89.38%\n",
      "Epoch: 75 | Train loss: 0.1180 | Train acc: 95.67% | Test loss: 0.5252 | Test acc: 89.44%\n",
      "Epoch: 76 | Train loss: 0.1238 | Train acc: 95.63% | Test loss: 0.5245 | Test acc: 88.77%\n",
      "Epoch: 77 | Train loss: 0.1257 | Train acc: 95.56% | Test loss: 0.5352 | Test acc: 89.03%\n",
      "Epoch: 78 | Train loss: 0.1196 | Train acc: 95.80% | Test loss: 0.5359 | Test acc: 89.26%\n",
      "Epoch: 79 | Train loss: 0.1214 | Train acc: 95.65% | Test loss: 0.5454 | Test acc: 88.62%\n",
      "Epoch: 80 | Train loss: 0.1187 | Train acc: 95.81% | Test loss: 0.5100 | Test acc: 89.07%\n",
      "Epoch: 81 | Train loss: 0.1177 | Train acc: 95.79% | Test loss: 0.5453 | Test acc: 88.77%\n",
      "Epoch: 82 | Train loss: 0.1155 | Train acc: 95.91% | Test loss: 0.5411 | Test acc: 89.12%\n",
      "Epoch: 83 | Train loss: 0.1148 | Train acc: 95.93% | Test loss: 0.5256 | Test acc: 89.71%\n",
      "Epoch: 84 | Train loss: 0.1167 | Train acc: 95.89% | Test loss: 0.5176 | Test acc: 89.62%\n",
      "Epoch: 85 | Train loss: 0.1243 | Train acc: 95.66% | Test loss: 0.5360 | Test acc: 89.34%\n",
      "Epoch: 86 | Train loss: 0.1191 | Train acc: 95.91% | Test loss: 0.5112 | Test acc: 89.48%\n",
      "Epoch: 87 | Train loss: 0.1195 | Train acc: 95.83% | Test loss: 0.5372 | Test acc: 89.84%\n",
      "Epoch: 88 | Train loss: 0.1184 | Train acc: 95.82% | Test loss: 0.5700 | Test acc: 88.77%\n",
      "Epoch: 89 | Train loss: 0.1172 | Train acc: 95.90% | Test loss: 0.5411 | Test acc: 89.10%\n",
      "Epoch: 90 | Train loss: 0.1191 | Train acc: 95.79% | Test loss: 0.5704 | Test acc: 88.84%\n",
      "Epoch: 91 | Train loss: 0.1211 | Train acc: 95.85% | Test loss: 0.5234 | Test acc: 89.49%\n",
      "Epoch: 92 | Train loss: 0.1140 | Train acc: 95.93% | Test loss: 0.5761 | Test acc: 89.28%\n",
      "Epoch: 93 | Train loss: 0.1202 | Train acc: 95.83% | Test loss: 0.5762 | Test acc: 89.18%\n",
      "Epoch: 94 | Train loss: 0.1239 | Train acc: 95.73% | Test loss: 0.5299 | Test acc: 88.99%\n",
      "Epoch: 95 | Train loss: 0.1201 | Train acc: 95.86% | Test loss: 0.5472 | Test acc: 89.54%\n",
      "Epoch: 96 | Train loss: 0.1129 | Train acc: 96.00% | Test loss: 0.5908 | Test acc: 88.68%\n",
      "Epoch: 97 | Train loss: 0.1124 | Train acc: 96.06% | Test loss: 0.5618 | Test acc: 88.98%\n",
      "Epoch: 98 | Train loss: 0.1075 | Train acc: 96.18% | Test loss: 0.6097 | Test acc: 89.13%\n",
      "Epoch: 99 | Train loss: 0.1245 | Train acc: 95.71% | Test loss: 0.5795 | Test acc: 88.95%\n",
      "Epoch: 100 | Train loss: 0.1143 | Train acc: 96.01% | Test loss: 0.5740 | Test acc: 89.17%\n",
      "Epoch: 101 | Train loss: 0.1133 | Train acc: 96.14% | Test loss: 0.5951 | Test acc: 88.79%\n",
      "Epoch: 102 | Train loss: 0.1166 | Train acc: 95.88% | Test loss: 0.6105 | Test acc: 89.24%\n",
      "Epoch: 103 | Train loss: 0.1179 | Train acc: 95.90% | Test loss: 0.6000 | Test acc: 89.37%\n",
      "Epoch: 104 | Train loss: 0.1284 | Train acc: 95.57% | Test loss: 0.6069 | Test acc: 89.22%\n",
      "Epoch: 105 | Train loss: 0.1183 | Train acc: 95.94% | Test loss: 0.6184 | Test acc: 88.47%\n",
      "Epoch: 106 | Train loss: 0.1215 | Train acc: 95.94% | Test loss: 0.5510 | Test acc: 89.43%\n",
      "Epoch: 107 | Train loss: 0.1224 | Train acc: 95.87% | Test loss: 0.5938 | Test acc: 89.33%\n",
      "Epoch: 108 | Train loss: 0.1178 | Train acc: 95.93% | Test loss: 0.5938 | Test acc: 89.53%\n",
      "Epoch: 109 | Train loss: 0.1164 | Train acc: 95.99% | Test loss: 0.6369 | Test acc: 88.44%\n",
      "Epoch: 110 | Train loss: 0.1151 | Train acc: 96.02% | Test loss: 0.6141 | Test acc: 89.09%\n",
      "Epoch: 111 | Train loss: 0.1108 | Train acc: 96.12% | Test loss: 0.6022 | Test acc: 89.42%\n",
      "Epoch: 112 | Train loss: 0.1185 | Train acc: 95.83% | Test loss: 0.6182 | Test acc: 88.74%\n",
      "Epoch: 113 | Train loss: 0.1211 | Train acc: 95.86% | Test loss: 0.6059 | Test acc: 89.77%\n",
      "Epoch: 114 | Train loss: 0.1172 | Train acc: 95.93% | Test loss: 0.5637 | Test acc: 88.77%\n",
      "Epoch: 115 | Train loss: 0.1169 | Train acc: 96.02% | Test loss: 0.5815 | Test acc: 88.82%\n",
      "Epoch: 116 | Train loss: 0.1204 | Train acc: 95.86% | Test loss: 0.6257 | Test acc: 89.62%\n",
      "Epoch: 117 | Train loss: 0.1232 | Train acc: 95.87% | Test loss: 0.5765 | Test acc: 88.96%\n",
      "Epoch: 118 | Train loss: 0.1242 | Train acc: 95.84% | Test loss: 0.6084 | Test acc: 89.19%\n",
      "Epoch: 119 | Train loss: 0.1190 | Train acc: 96.01% | Test loss: 0.6055 | Test acc: 89.10%\n",
      "Epoch: 120 | Train loss: 0.1179 | Train acc: 95.97% | Test loss: 0.6118 | Test acc: 89.21%\n",
      "Epoch: 121 | Train loss: 0.1101 | Train acc: 96.24% | Test loss: 0.6263 | Test acc: 89.28%\n",
      "Epoch: 122 | Train loss: 0.1196 | Train acc: 95.93% | Test loss: 0.6572 | Test acc: 88.62%\n",
      "Epoch: 123 | Train loss: 0.1206 | Train acc: 95.92% | Test loss: 0.6150 | Test acc: 88.70%\n",
      "Epoch: 124 | Train loss: 0.1130 | Train acc: 96.13% | Test loss: 0.6126 | Test acc: 89.13%\n",
      "Epoch: 125 | Train loss: 0.1209 | Train acc: 95.94% | Test loss: 0.6003 | Test acc: 89.18%\n",
      "Epoch: 126 | Train loss: 0.1161 | Train acc: 95.98% | Test loss: 0.6449 | Test acc: 88.63%\n",
      "Epoch: 127 | Train loss: 0.1189 | Train acc: 95.99% | Test loss: 0.6206 | Test acc: 88.85%\n",
      "Epoch: 128 | Train loss: 0.1195 | Train acc: 96.05% | Test loss: 0.6127 | Test acc: 89.54%\n",
      "Epoch: 129 | Train loss: 0.1069 | Train acc: 96.29% | Test loss: 0.6558 | Test acc: 88.77%\n",
      "Epoch: 130 | Train loss: 0.1131 | Train acc: 96.22% | Test loss: 0.6294 | Test acc: 88.78%\n",
      "Epoch: 131 | Train loss: 0.1102 | Train acc: 96.20% | Test loss: 0.6247 | Test acc: 88.65%\n",
      "Epoch: 132 | Train loss: 0.1156 | Train acc: 96.09% | Test loss: 0.6513 | Test acc: 89.20%\n",
      "Epoch: 133 | Train loss: 0.1283 | Train acc: 95.80% | Test loss: 0.6243 | Test acc: 89.02%\n",
      "Epoch: 134 | Train loss: 0.1179 | Train acc: 96.06% | Test loss: 0.6277 | Test acc: 89.26%\n",
      "Epoch: 135 | Train loss: 0.1205 | Train acc: 95.98% | Test loss: 0.6000 | Test acc: 88.77%\n",
      "Epoch: 136 | Train loss: 0.1199 | Train acc: 95.98% | Test loss: 0.6482 | Test acc: 89.08%\n",
      "Epoch: 137 | Train loss: 0.1209 | Train acc: 96.06% | Test loss: 0.6332 | Test acc: 89.00%\n",
      "Epoch: 138 | Train loss: 0.1207 | Train acc: 95.98% | Test loss: 0.6322 | Test acc: 88.91%\n",
      "Epoch: 139 | Train loss: 0.1174 | Train acc: 96.13% | Test loss: 0.6220 | Test acc: 89.20%\n",
      "Epoch: 140 | Train loss: 0.1245 | Train acc: 95.91% | Test loss: 0.7074 | Test acc: 89.09%\n",
      "Epoch: 141 | Train loss: 0.1252 | Train acc: 95.88% | Test loss: 0.6368 | Test acc: 89.19%\n",
      "Epoch: 142 | Train loss: 0.1115 | Train acc: 96.33% | Test loss: 0.6298 | Test acc: 88.95%\n",
      "Epoch: 143 | Train loss: 0.1151 | Train acc: 96.25% | Test loss: 0.6783 | Test acc: 88.24%\n",
      "Epoch: 144 | Train loss: 0.1155 | Train acc: 96.12% | Test loss: 0.6306 | Test acc: 89.40%\n",
      "Epoch: 145 | Train loss: 0.1192 | Train acc: 96.07% | Test loss: 0.6724 | Test acc: 89.05%\n",
      "Epoch: 146 | Train loss: 0.1129 | Train acc: 96.17% | Test loss: 0.6703 | Test acc: 88.67%\n",
      "Epoch: 147 | Train loss: 0.1238 | Train acc: 95.97% | Test loss: 0.6420 | Test acc: 89.00%\n",
      "Epoch: 148 | Train loss: 0.1194 | Train acc: 96.05% | Test loss: 0.6680 | Test acc: 89.41%\n",
      "Epoch: 149 | Train loss: 0.1147 | Train acc: 96.28% | Test loss: 0.6707 | Test acc: 89.41%\n",
      "Epoch: 150 | Train loss: 0.1245 | Train acc: 95.95% | Test loss: 0.6698 | Test acc: 89.24%\n",
      "Epoch: 151 | Train loss: 0.1138 | Train acc: 96.26% | Test loss: 0.6397 | Test acc: 89.39%\n",
      "Epoch: 152 | Train loss: 0.1204 | Train acc: 96.02% | Test loss: 0.6414 | Test acc: 89.11%\n",
      "Epoch: 153 | Train loss: 0.1263 | Train acc: 95.96% | Test loss: 0.6591 | Test acc: 88.87%\n",
      "Epoch: 154 | Train loss: 0.1266 | Train acc: 95.90% | Test loss: 0.6935 | Test acc: 88.56%\n",
      "Epoch: 155 | Train loss: 0.1142 | Train acc: 96.16% | Test loss: 0.7355 | Test acc: 87.99%\n",
      "Epoch: 156 | Train loss: 0.1158 | Train acc: 96.16% | Test loss: 0.6637 | Test acc: 88.74%\n",
      "Epoch: 157 | Train loss: 0.1247 | Train acc: 95.91% | Test loss: 0.6188 | Test acc: 88.88%\n",
      "Epoch: 158 | Train loss: 0.1269 | Train acc: 95.85% | Test loss: 0.6670 | Test acc: 89.00%\n",
      "Epoch: 159 | Train loss: 0.1128 | Train acc: 96.16% | Test loss: 0.7005 | Test acc: 88.51%\n",
      "Epoch: 160 | Train loss: 0.1187 | Train acc: 96.22% | Test loss: 0.6888 | Test acc: 88.86%\n",
      "Epoch: 161 | Train loss: 0.1183 | Train acc: 96.12% | Test loss: 0.6449 | Test acc: 88.98%\n",
      "Epoch: 162 | Train loss: 0.1230 | Train acc: 96.06% | Test loss: 0.6816 | Test acc: 87.93%\n",
      "Epoch: 163 | Train loss: 0.1174 | Train acc: 96.17% | Test loss: 0.6714 | Test acc: 89.09%\n",
      "Epoch: 164 | Train loss: 0.1172 | Train acc: 96.19% | Test loss: 0.7559 | Test acc: 88.07%\n",
      "Epoch: 165 | Train loss: 0.1338 | Train acc: 95.73% | Test loss: 0.7581 | Test acc: 88.62%\n",
      "Epoch: 166 | Train loss: 0.1225 | Train acc: 96.00% | Test loss: 0.6731 | Test acc: 89.40%\n",
      "Epoch: 167 | Train loss: 0.1125 | Train acc: 96.33% | Test loss: 0.6449 | Test acc: 89.06%\n",
      "Epoch: 168 | Train loss: 0.1163 | Train acc: 96.10% | Test loss: 0.7415 | Test acc: 89.33%\n",
      "Epoch: 169 | Train loss: 0.1209 | Train acc: 96.06% | Test loss: 0.7273 | Test acc: 89.00%\n",
      "Epoch: 170 | Train loss: 0.1255 | Train acc: 96.00% | Test loss: 0.7208 | Test acc: 89.31%\n",
      "Epoch: 171 | Train loss: 0.1225 | Train acc: 96.00% | Test loss: 0.6665 | Test acc: 89.16%\n",
      "Epoch: 172 | Train loss: 0.1122 | Train acc: 96.31% | Test loss: 0.6983 | Test acc: 88.76%\n",
      "Epoch: 173 | Train loss: 0.1221 | Train acc: 96.07% | Test loss: 0.7022 | Test acc: 88.66%\n",
      "Epoch: 174 | Train loss: 0.1293 | Train acc: 95.85% | Test loss: 0.6614 | Test acc: 89.03%\n",
      "Epoch: 175 | Train loss: 0.1227 | Train acc: 95.98% | Test loss: 0.6990 | Test acc: 88.58%\n",
      "Epoch: 176 | Train loss: 0.1251 | Train acc: 95.92% | Test loss: 0.6779 | Test acc: 88.58%\n",
      "Epoch: 177 | Train loss: 0.1166 | Train acc: 96.18% | Test loss: 0.7010 | Test acc: 88.97%\n",
      "Epoch: 178 | Train loss: 0.1199 | Train acc: 96.07% | Test loss: 0.7233 | Test acc: 89.02%\n",
      "Epoch: 179 | Train loss: 0.1168 | Train acc: 96.26% | Test loss: 0.7200 | Test acc: 88.63%\n",
      "Epoch: 180 | Train loss: 0.1127 | Train acc: 96.26% | Test loss: 0.7382 | Test acc: 88.20%\n",
      "Epoch: 181 | Train loss: 0.1166 | Train acc: 96.17% | Test loss: 0.7083 | Test acc: 89.44%\n",
      "Epoch: 182 | Train loss: 0.1150 | Train acc: 96.30% | Test loss: 0.7282 | Test acc: 89.07%\n",
      "Epoch: 183 | Train loss: 0.1238 | Train acc: 96.12% | Test loss: 0.7203 | Test acc: 88.84%\n",
      "Epoch: 184 | Train loss: 0.1170 | Train acc: 96.16% | Test loss: 0.7134 | Test acc: 88.78%\n",
      "Epoch: 185 | Train loss: 0.1248 | Train acc: 96.04% | Test loss: 0.7550 | Test acc: 88.80%\n",
      "Epoch: 186 | Train loss: 0.1293 | Train acc: 95.88% | Test loss: 0.6918 | Test acc: 89.45%\n",
      "Epoch: 187 | Train loss: 0.1318 | Train acc: 95.81% | Test loss: 0.7136 | Test acc: 88.55%\n",
      "Epoch: 188 | Train loss: 0.1216 | Train acc: 96.14% | Test loss: 0.7913 | Test acc: 89.02%\n",
      "Epoch: 189 | Train loss: 0.1140 | Train acc: 96.23% | Test loss: 0.7006 | Test acc: 88.84%\n",
      "Epoch: 190 | Train loss: 0.1232 | Train acc: 96.08% | Test loss: 0.7839 | Test acc: 88.48%\n",
      "Epoch: 191 | Train loss: 0.1169 | Train acc: 96.27% | Test loss: 0.7201 | Test acc: 88.83%\n",
      "Epoch: 192 | Train loss: 0.1153 | Train acc: 96.28% | Test loss: 0.7207 | Test acc: 89.03%\n",
      "Epoch: 193 | Train loss: 0.1160 | Train acc: 96.19% | Test loss: 0.7188 | Test acc: 88.24%\n",
      "Epoch: 194 | Train loss: 0.1165 | Train acc: 96.30% | Test loss: 0.7680 | Test acc: 88.84%\n",
      "Epoch: 195 | Train loss: 0.1148 | Train acc: 96.25% | Test loss: 0.7687 | Test acc: 88.61%\n",
      "Epoch: 196 | Train loss: 0.1149 | Train acc: 96.31% | Test loss: 0.7067 | Test acc: 89.29%\n",
      "Epoch: 197 | Train loss: 0.1204 | Train acc: 96.16% | Test loss: 0.7387 | Test acc: 89.07%\n",
      "Epoch: 198 | Train loss: 0.1356 | Train acc: 95.82% | Test loss: 0.7089 | Test acc: 89.06%\n",
      "Epoch: 199 | Train loss: 0.1160 | Train acc: 96.36% | Test loss: 0.7240 | Test acc: 89.17%\n",
      "Epoch: 200 | Train loss: 0.1215 | Train acc: 96.10% | Test loss: 0.7091 | Test acc: 88.92%\n",
      "Epoch: 201 | Train loss: 0.1141 | Train acc: 96.40% | Test loss: 0.7443 | Test acc: 89.19%\n",
      "Epoch: 202 | Train loss: 0.1133 | Train acc: 96.43% | Test loss: 0.7391 | Test acc: 88.89%\n",
      "Epoch: 203 | Train loss: 0.1150 | Train acc: 96.28% | Test loss: 0.7306 | Test acc: 89.11%\n",
      "Epoch: 204 | Train loss: 0.1215 | Train acc: 96.30% | Test loss: 0.7234 | Test acc: 88.65%\n",
      "Epoch: 205 | Train loss: 0.1251 | Train acc: 96.08% | Test loss: 0.8067 | Test acc: 87.54%\n",
      "Epoch: 206 | Train loss: 0.1128 | Train acc: 96.34% | Test loss: 0.7648 | Test acc: 89.42%\n",
      "Epoch: 207 | Train loss: 0.1173 | Train acc: 96.38% | Test loss: 0.7395 | Test acc: 88.41%\n",
      "Epoch: 208 | Train loss: 0.1190 | Train acc: 96.26% | Test loss: 0.8413 | Test acc: 87.96%\n",
      "Epoch: 209 | Train loss: 0.1192 | Train acc: 96.31% | Test loss: 0.7837 | Test acc: 89.10%\n",
      "Epoch: 210 | Train loss: 0.1210 | Train acc: 96.16% | Test loss: 0.7480 | Test acc: 88.64%\n",
      "Epoch: 211 | Train loss: 0.1272 | Train acc: 96.07% | Test loss: 0.8330 | Test acc: 87.85%\n",
      "Epoch: 212 | Train loss: 0.1244 | Train acc: 96.05% | Test loss: 0.8120 | Test acc: 89.07%\n",
      "Epoch: 213 | Train loss: 0.1257 | Train acc: 96.07% | Test loss: 0.7467 | Test acc: 89.06%\n",
      "Epoch: 214 | Train loss: 0.1200 | Train acc: 96.32% | Test loss: 0.8424 | Test acc: 88.34%\n",
      "Epoch: 215 | Train loss: 0.1339 | Train acc: 95.92% | Test loss: 0.7644 | Test acc: 87.72%\n",
      "Epoch: 216 | Train loss: 0.1270 | Train acc: 96.05% | Test loss: 0.7459 | Test acc: 89.13%\n",
      "Epoch: 217 | Train loss: 0.1150 | Train acc: 96.34% | Test loss: 0.7526 | Test acc: 88.59%\n",
      "Epoch: 218 | Train loss: 0.1196 | Train acc: 96.34% | Test loss: 0.7802 | Test acc: 88.58%\n",
      "Epoch: 219 | Train loss: 0.1252 | Train acc: 96.13% | Test loss: 0.7219 | Test acc: 88.97%\n",
      "Epoch: 220 | Train loss: 0.1177 | Train acc: 96.36% | Test loss: 0.7953 | Test acc: 88.99%\n",
      "Epoch: 221 | Train loss: 0.1266 | Train acc: 96.14% | Test loss: 0.7791 | Test acc: 89.07%\n",
      "Epoch: 222 | Train loss: 0.1277 | Train acc: 96.08% | Test loss: 0.7479 | Test acc: 89.51%\n",
      "Epoch: 223 | Train loss: 0.1303 | Train acc: 96.06% | Test loss: 0.8075 | Test acc: 88.54%\n",
      "Epoch: 224 | Train loss: 0.1300 | Train acc: 96.04% | Test loss: 0.8100 | Test acc: 88.64%\n",
      "Epoch: 225 | Train loss: 0.1257 | Train acc: 95.98% | Test loss: 0.8056 | Test acc: 88.90%\n",
      "Epoch: 226 | Train loss: 0.1234 | Train acc: 96.23% | Test loss: 0.8185 | Test acc: 89.10%\n",
      "Epoch: 227 | Train loss: 0.1190 | Train acc: 96.20% | Test loss: 0.8330 | Test acc: 88.41%\n",
      "Epoch: 228 | Train loss: 0.1312 | Train acc: 95.98% | Test loss: 0.7709 | Test acc: 88.82%\n",
      "Epoch: 229 | Train loss: 0.1162 | Train acc: 96.31% | Test loss: 0.7743 | Test acc: 88.26%\n",
      "Epoch: 230 | Train loss: 0.1110 | Train acc: 96.55% | Test loss: 0.8171 | Test acc: 89.33%\n",
      "Epoch: 231 | Train loss: 0.1243 | Train acc: 96.14% | Test loss: 0.7822 | Test acc: 88.39%\n",
      "Epoch: 232 | Train loss: 0.1257 | Train acc: 96.21% | Test loss: 0.7918 | Test acc: 88.86%\n",
      "Epoch: 233 | Train loss: 0.1304 | Train acc: 96.16% | Test loss: 0.8135 | Test acc: 88.16%\n",
      "Epoch: 234 | Train loss: 0.1287 | Train acc: 96.06% | Test loss: 0.7628 | Test acc: 88.69%\n",
      "Epoch: 235 | Train loss: 0.1295 | Train acc: 96.16% | Test loss: 0.7664 | Test acc: 89.28%\n",
      "Epoch: 236 | Train loss: 0.1305 | Train acc: 96.09% | Test loss: 0.8509 | Test acc: 87.99%\n",
      "Epoch: 237 | Train loss: 0.1324 | Train acc: 95.97% | Test loss: 0.8247 | Test acc: 88.64%\n",
      "Epoch: 238 | Train loss: 0.1197 | Train acc: 96.21% | Test loss: 0.8248 | Test acc: 88.94%\n",
      "Epoch: 239 | Train loss: 0.1130 | Train acc: 96.50% | Test loss: 0.8975 | Test acc: 89.04%\n",
      "Epoch: 240 | Train loss: 0.1298 | Train acc: 96.08% | Test loss: 0.7939 | Test acc: 89.00%\n",
      "Epoch: 241 | Train loss: 0.1204 | Train acc: 96.31% | Test loss: 0.8554 | Test acc: 88.72%\n",
      "Epoch: 242 | Train loss: 0.1209 | Train acc: 96.35% | Test loss: 0.8126 | Test acc: 88.87%\n",
      "Epoch: 243 | Train loss: 0.1261 | Train acc: 96.18% | Test loss: 0.8261 | Test acc: 89.14%\n",
      "Epoch: 244 | Train loss: 0.1227 | Train acc: 96.21% | Test loss: 0.8432 | Test acc: 88.11%\n",
      "Epoch: 245 | Train loss: 0.1223 | Train acc: 96.19% | Test loss: 0.7638 | Test acc: 88.61%\n",
      "Epoch: 246 | Train loss: 0.1279 | Train acc: 96.08% | Test loss: 0.8526 | Test acc: 88.71%\n",
      "Epoch: 247 | Train loss: 0.1293 | Train acc: 96.11% | Test loss: 0.8816 | Test acc: 87.77%\n",
      "Epoch: 248 | Train loss: 0.1205 | Train acc: 96.29% | Test loss: 0.8399 | Test acc: 89.23%\n",
      "Epoch: 249 | Train loss: 0.1270 | Train acc: 96.10% | Test loss: 0.8231 | Test acc: 89.17%\n",
      "Epoch: 250 | Train loss: 0.1227 | Train acc: 96.21% | Test loss: 0.9017 | Test acc: 88.71%\n",
      "Epoch: 251 | Train loss: 0.1296 | Train acc: 96.12% | Test loss: 0.8299 | Test acc: 88.96%\n",
      "Epoch: 252 | Train loss: 0.1281 | Train acc: 96.12% | Test loss: 0.8601 | Test acc: 88.77%\n",
      "Epoch: 253 | Train loss: 0.1211 | Train acc: 96.27% | Test loss: 0.7944 | Test acc: 88.39%\n",
      "Epoch: 254 | Train loss: 0.1289 | Train acc: 96.15% | Test loss: 0.8631 | Test acc: 88.95%\n",
      "Epoch: 255 | Train loss: 0.1236 | Train acc: 96.23% | Test loss: 0.8384 | Test acc: 89.00%\n",
      "Epoch: 256 | Train loss: 0.1177 | Train acc: 96.47% | Test loss: 0.8071 | Test acc: 89.24%\n",
      "Epoch: 257 | Train loss: 0.1306 | Train acc: 96.14% | Test loss: 0.8604 | Test acc: 88.60%\n",
      "Epoch: 258 | Train loss: 0.1302 | Train acc: 96.03% | Test loss: 0.7633 | Test acc: 88.45%\n",
      "Epoch: 259 | Train loss: 0.1199 | Train acc: 96.32% | Test loss: 0.7834 | Test acc: 88.64%\n",
      "Epoch: 260 | Train loss: 0.1264 | Train acc: 96.20% | Test loss: 0.8465 | Test acc: 89.10%\n",
      "Epoch: 261 | Train loss: 0.1068 | Train acc: 96.70% | Test loss: 0.8685 | Test acc: 89.29%\n",
      "Epoch: 262 | Train loss: 0.1093 | Train acc: 96.66% | Test loss: 0.8400 | Test acc: 89.19%\n",
      "Epoch: 263 | Train loss: 0.1289 | Train acc: 96.19% | Test loss: 0.8689 | Test acc: 88.65%\n",
      "Epoch: 264 | Train loss: 0.1259 | Train acc: 96.24% | Test loss: 0.8688 | Test acc: 88.70%\n",
      "Epoch: 265 | Train loss: 0.1267 | Train acc: 96.23% | Test loss: 0.8108 | Test acc: 88.59%\n",
      "Epoch: 266 | Train loss: 0.1190 | Train acc: 96.34% | Test loss: 0.9058 | Test acc: 88.43%\n",
      "Epoch: 267 | Train loss: 0.1250 | Train acc: 96.19% | Test loss: 0.8669 | Test acc: 88.84%\n",
      "Epoch: 268 | Train loss: 0.1261 | Train acc: 96.25% | Test loss: 0.9381 | Test acc: 88.44%\n",
      "Epoch: 269 | Train loss: 0.1388 | Train acc: 95.91% | Test loss: 0.8564 | Test acc: 89.13%\n",
      "Epoch: 270 | Train loss: 0.1177 | Train acc: 96.40% | Test loss: 0.8808 | Test acc: 88.21%\n",
      "Epoch: 271 | Train loss: 0.1342 | Train acc: 96.02% | Test loss: 0.8484 | Test acc: 88.38%\n",
      "Epoch: 272 | Train loss: 0.1260 | Train acc: 96.21% | Test loss: 0.8100 | Test acc: 87.95%\n",
      "Epoch: 273 | Train loss: 0.1208 | Train acc: 96.37% | Test loss: 0.8681 | Test acc: 88.35%\n",
      "Epoch: 274 | Train loss: 0.1318 | Train acc: 96.10% | Test loss: 0.8152 | Test acc: 88.35%\n",
      "Epoch: 275 | Train loss: 0.1392 | Train acc: 95.87% | Test loss: 0.9011 | Test acc: 88.37%\n",
      "Epoch: 276 | Train loss: 0.1416 | Train acc: 95.89% | Test loss: 0.8206 | Test acc: 88.47%\n",
      "Epoch: 277 | Train loss: 0.1290 | Train acc: 96.21% | Test loss: 0.9522 | Test acc: 87.92%\n",
      "Epoch: 278 | Train loss: 0.1325 | Train acc: 96.07% | Test loss: 0.8586 | Test acc: 88.73%\n",
      "Epoch: 279 | Train loss: 0.1293 | Train acc: 96.13% | Test loss: 0.7936 | Test acc: 89.09%\n",
      "Epoch: 280 | Train loss: 0.1147 | Train acc: 96.39% | Test loss: 0.8229 | Test acc: 89.04%\n",
      "Epoch: 281 | Train loss: 0.1149 | Train acc: 96.51% | Test loss: 0.8333 | Test acc: 89.10%\n",
      "Epoch: 282 | Train loss: 0.1125 | Train acc: 96.54% | Test loss: 0.8286 | Test acc: 88.54%\n",
      "Epoch: 283 | Train loss: 0.1309 | Train acc: 96.13% | Test loss: 0.8473 | Test acc: 89.12%\n",
      "Epoch: 284 | Train loss: 0.1275 | Train acc: 96.22% | Test loss: 0.8481 | Test acc: 88.39%\n",
      "Epoch: 285 | Train loss: 0.1410 | Train acc: 95.89% | Test loss: 0.9226 | Test acc: 88.56%\n",
      "Epoch: 286 | Train loss: 0.1309 | Train acc: 96.08% | Test loss: 0.7870 | Test acc: 88.60%\n",
      "Epoch: 287 | Train loss: 0.1168 | Train acc: 96.50% | Test loss: 0.9527 | Test acc: 88.55%\n",
      "Epoch: 288 | Train loss: 0.1200 | Train acc: 96.37% | Test loss: 0.9058 | Test acc: 88.53%\n",
      "Epoch: 289 | Train loss: 0.1132 | Train acc: 96.61% | Test loss: 0.8711 | Test acc: 89.06%\n",
      "Epoch: 290 | Train loss: 0.1174 | Train acc: 96.55% | Test loss: 0.9489 | Test acc: 88.77%\n",
      "Epoch: 291 | Train loss: 0.1333 | Train acc: 96.09% | Test loss: 0.8691 | Test acc: 88.54%\n",
      "Epoch: 292 | Train loss: 0.1303 | Train acc: 96.12% | Test loss: 0.9056 | Test acc: 88.72%\n",
      "Epoch: 293 | Train loss: 0.1344 | Train acc: 96.15% | Test loss: 0.8678 | Test acc: 88.09%\n",
      "Epoch: 294 | Train loss: 0.1336 | Train acc: 96.17% | Test loss: 0.8805 | Test acc: 89.36%\n",
      "Epoch: 295 | Train loss: 0.1288 | Train acc: 96.21% | Test loss: 0.8874 | Test acc: 88.52%\n",
      "Epoch: 296 | Train loss: 0.1388 | Train acc: 95.93% | Test loss: 0.8832 | Test acc: 88.20%\n",
      "Epoch: 297 | Train loss: 0.1266 | Train acc: 96.25% | Test loss: 0.9418 | Test acc: 88.68%\n",
      "Epoch: 298 | Train loss: 0.1266 | Train acc: 96.28% | Test loss: 0.9203 | Test acc: 89.03%\n",
      "Epoch: 299 | Train loss: 0.1412 | Train acc: 95.88% | Test loss: 0.8381 | Test acc: 88.38%\n",
      "Epoch: 300 | Train loss: 0.1301 | Train acc: 96.16% | Test loss: 0.9268 | Test acc: 88.32%\n",
      "Epoch: 301 | Train loss: 0.1322 | Train acc: 96.19% | Test loss: 0.9685 | Test acc: 88.55%\n",
      "Epoch: 302 | Train loss: 0.1352 | Train acc: 96.05% | Test loss: 0.8334 | Test acc: 89.03%\n",
      "Epoch: 303 | Train loss: 0.1141 | Train acc: 96.46% | Test loss: 0.9157 | Test acc: 88.40%\n",
      "Epoch: 304 | Train loss: 0.1231 | Train acc: 96.34% | Test loss: 0.8926 | Test acc: 89.30%\n",
      "Epoch: 305 | Train loss: 0.1311 | Train acc: 96.16% | Test loss: 0.9678 | Test acc: 88.90%\n",
      "Epoch: 306 | Train loss: 0.1400 | Train acc: 95.98% | Test loss: 0.8748 | Test acc: 88.32%\n",
      "Epoch: 307 | Train loss: 0.1438 | Train acc: 95.86% | Test loss: 0.8785 | Test acc: 87.96%\n",
      "Epoch: 308 | Train loss: 0.1331 | Train acc: 96.02% | Test loss: 0.8619 | Test acc: 88.12%\n",
      "Epoch: 309 | Train loss: 0.1360 | Train acc: 95.99% | Test loss: 0.8207 | Test acc: 88.64%\n",
      "Epoch: 310 | Train loss: 0.1246 | Train acc: 96.41% | Test loss: 0.8978 | Test acc: 88.12%\n",
      "Epoch: 311 | Train loss: 0.1334 | Train acc: 96.00% | Test loss: 0.8945 | Test acc: 87.92%\n",
      "Epoch: 312 | Train loss: 0.1198 | Train acc: 96.48% | Test loss: 0.8421 | Test acc: 88.28%\n",
      "Epoch: 313 | Train loss: 0.1162 | Train acc: 96.44% | Test loss: 0.9126 | Test acc: 88.99%\n",
      "Epoch: 314 | Train loss: 0.1171 | Train acc: 96.52% | Test loss: 0.9050 | Test acc: 88.55%\n",
      "Epoch: 315 | Train loss: 0.1401 | Train acc: 95.90% | Test loss: 0.8666 | Test acc: 88.63%\n",
      "Epoch: 316 | Train loss: 0.1122 | Train acc: 96.66% | Test loss: 0.8936 | Test acc: 88.92%\n",
      "Epoch: 317 | Train loss: 0.1407 | Train acc: 96.03% | Test loss: 0.8481 | Test acc: 87.91%\n",
      "Epoch: 318 | Train loss: 0.1495 | Train acc: 95.75% | Test loss: 0.8815 | Test acc: 88.60%\n",
      "Epoch: 319 | Train loss: 0.1337 | Train acc: 96.02% | Test loss: 0.8958 | Test acc: 88.52%\n",
      "Epoch: 320 | Train loss: 0.1286 | Train acc: 96.17% | Test loss: 0.8461 | Test acc: 88.50%\n",
      "Epoch: 321 | Train loss: 0.1273 | Train acc: 96.24% | Test loss: 0.8661 | Test acc: 88.51%\n",
      "Epoch: 322 | Train loss: 0.1282 | Train acc: 96.20% | Test loss: 0.9047 | Test acc: 88.20%\n",
      "Epoch: 323 | Train loss: 0.1392 | Train acc: 96.00% | Test loss: 0.8565 | Test acc: 88.76%\n",
      "Epoch: 324 | Train loss: 0.1236 | Train acc: 96.40% | Test loss: 0.9233 | Test acc: 87.96%\n",
      "Epoch: 325 | Train loss: 0.1216 | Train acc: 96.36% | Test loss: 0.9090 | Test acc: 88.93%\n",
      "Epoch: 326 | Train loss: 0.1309 | Train acc: 96.19% | Test loss: 0.9626 | Test acc: 88.76%\n",
      "Epoch: 327 | Train loss: 0.1465 | Train acc: 95.85% | Test loss: 0.9503 | Test acc: 88.62%\n",
      "Epoch: 328 | Train loss: 0.1229 | Train acc: 96.27% | Test loss: 0.8860 | Test acc: 88.52%\n",
      "Epoch: 329 | Train loss: 0.1246 | Train acc: 96.33% | Test loss: 0.9399 | Test acc: 88.29%\n",
      "Epoch: 330 | Train loss: 0.1236 | Train acc: 96.28% | Test loss: 0.9316 | Test acc: 89.06%\n",
      "Epoch: 331 | Train loss: 0.1241 | Train acc: 96.36% | Test loss: 0.9407 | Test acc: 88.82%\n",
      "Epoch: 332 | Train loss: 0.1229 | Train acc: 96.25% | Test loss: 0.8719 | Test acc: 88.27%\n",
      "Epoch: 333 | Train loss: 0.1213 | Train acc: 96.40% | Test loss: 0.9250 | Test acc: 87.38%\n",
      "Epoch: 334 | Train loss: 0.1326 | Train acc: 96.16% | Test loss: 0.8631 | Test acc: 88.12%\n",
      "Epoch: 335 | Train loss: 0.1420 | Train acc: 95.91% | Test loss: 0.9159 | Test acc: 88.65%\n",
      "Epoch: 336 | Train loss: 0.1352 | Train acc: 96.05% | Test loss: 0.8844 | Test acc: 88.54%\n",
      "Epoch: 337 | Train loss: 0.1370 | Train acc: 96.03% | Test loss: 0.9659 | Test acc: 88.31%\n",
      "Epoch: 338 | Train loss: 0.1312 | Train acc: 96.22% | Test loss: 0.8736 | Test acc: 88.88%\n",
      "Epoch: 339 | Train loss: 0.1263 | Train acc: 96.29% | Test loss: 0.8932 | Test acc: 88.17%\n",
      "Epoch: 340 | Train loss: 0.1229 | Train acc: 96.39% | Test loss: 0.8742 | Test acc: 88.66%\n",
      "Epoch: 341 | Train loss: 0.1281 | Train acc: 96.19% | Test loss: 0.9566 | Test acc: 89.12%\n",
      "Epoch: 342 | Train loss: 0.1253 | Train acc: 96.35% | Test loss: 0.9525 | Test acc: 88.28%\n",
      "Epoch: 343 | Train loss: 0.1368 | Train acc: 96.15% | Test loss: 0.9525 | Test acc: 88.43%\n",
      "Epoch: 344 | Train loss: 0.1388 | Train acc: 96.04% | Test loss: 0.8688 | Test acc: 88.56%\n",
      "Epoch: 345 | Train loss: 0.1333 | Train acc: 96.06% | Test loss: 0.8878 | Test acc: 88.90%\n",
      "Epoch: 346 | Train loss: 0.1250 | Train acc: 96.34% | Test loss: 0.9814 | Test acc: 88.73%\n",
      "Epoch: 347 | Train loss: 0.1189 | Train acc: 96.45% | Test loss: 0.9654 | Test acc: 88.40%\n",
      "Epoch: 348 | Train loss: 0.1392 | Train acc: 96.04% | Test loss: 0.8848 | Test acc: 88.67%\n",
      "Epoch: 349 | Train loss: 0.1288 | Train acc: 96.22% | Test loss: 0.9351 | Test acc: 88.66%\n",
      "Epoch: 350 | Train loss: 0.1343 | Train acc: 96.09% | Test loss: 0.9505 | Test acc: 88.66%\n",
      "Epoch: 351 | Train loss: 0.1281 | Train acc: 96.29% | Test loss: 0.9242 | Test acc: 88.59%\n",
      "Epoch: 352 | Train loss: 0.1402 | Train acc: 96.12% | Test loss: 0.9322 | Test acc: 88.02%\n",
      "Epoch: 353 | Train loss: 0.1320 | Train acc: 96.11% | Test loss: 0.9550 | Test acc: 88.31%\n",
      "Epoch: 354 | Train loss: 0.1209 | Train acc: 96.40% | Test loss: 0.8783 | Test acc: 88.86%\n",
      "Epoch: 355 | Train loss: 0.1158 | Train acc: 96.54% | Test loss: 0.9890 | Test acc: 88.68%\n",
      "Epoch: 356 | Train loss: 0.1185 | Train acc: 96.49% | Test loss: 0.8934 | Test acc: 88.48%\n",
      "Epoch: 357 | Train loss: 0.1179 | Train acc: 96.56% | Test loss: 1.0653 | Test acc: 88.48%\n",
      "Epoch: 358 | Train loss: 0.1430 | Train acc: 95.94% | Test loss: 0.9021 | Test acc: 88.52%\n",
      "Epoch: 359 | Train loss: 0.1335 | Train acc: 96.14% | Test loss: 0.9488 | Test acc: 88.82%\n",
      "Epoch: 360 | Train loss: 0.1341 | Train acc: 96.13% | Test loss: 1.0095 | Test acc: 88.44%\n",
      "Epoch: 361 | Train loss: 0.1526 | Train acc: 95.81% | Test loss: 0.9078 | Test acc: 88.26%\n",
      "Epoch: 362 | Train loss: 0.1377 | Train acc: 96.00% | Test loss: 0.9176 | Test acc: 88.86%\n",
      "Epoch: 363 | Train loss: 0.1265 | Train acc: 96.36% | Test loss: 0.9552 | Test acc: 88.51%\n",
      "Epoch: 364 | Train loss: 0.1405 | Train acc: 96.03% | Test loss: 0.9216 | Test acc: 88.70%\n",
      "Epoch: 365 | Train loss: 0.1178 | Train acc: 96.57% | Test loss: 0.9489 | Test acc: 88.33%\n",
      "Epoch: 366 | Train loss: 0.1288 | Train acc: 96.34% | Test loss: 0.9058 | Test acc: 89.02%\n",
      "Epoch: 367 | Train loss: 0.1243 | Train acc: 96.33% | Test loss: 0.9416 | Test acc: 88.45%\n",
      "Epoch: 368 | Train loss: 0.1117 | Train acc: 96.69% | Test loss: 1.0159 | Test acc: 88.80%\n",
      "Epoch: 369 | Train loss: 0.1278 | Train acc: 96.33% | Test loss: 0.9714 | Test acc: 89.14%\n",
      "Epoch: 370 | Train loss: 0.1381 | Train acc: 96.11% | Test loss: 0.9023 | Test acc: 88.74%\n",
      "Epoch: 371 | Train loss: 0.1312 | Train acc: 96.29% | Test loss: 0.9295 | Test acc: 88.66%\n",
      "Epoch: 372 | Train loss: 0.1257 | Train acc: 96.32% | Test loss: 1.0168 | Test acc: 89.29%\n",
      "Epoch: 373 | Train loss: 0.1229 | Train acc: 96.52% | Test loss: 0.9976 | Test acc: 88.61%\n",
      "Epoch: 374 | Train loss: 0.1381 | Train acc: 96.15% | Test loss: 0.8989 | Test acc: 88.61%\n",
      "Epoch: 375 | Train loss: 0.1386 | Train acc: 96.06% | Test loss: 0.9226 | Test acc: 89.23%\n",
      "Epoch: 376 | Train loss: 0.1288 | Train acc: 96.37% | Test loss: 0.9673 | Test acc: 88.22%\n",
      "Epoch: 377 | Train loss: 0.1339 | Train acc: 96.25% | Test loss: 1.0427 | Test acc: 88.57%\n",
      "Epoch: 378 | Train loss: 0.1434 | Train acc: 96.02% | Test loss: 0.8871 | Test acc: 88.38%\n",
      "Epoch: 379 | Train loss: 0.1269 | Train acc: 96.31% | Test loss: 0.9380 | Test acc: 88.66%\n",
      "Epoch: 380 | Train loss: 0.1332 | Train acc: 96.14% | Test loss: 0.9879 | Test acc: 88.27%\n",
      "Epoch: 381 | Train loss: 0.1392 | Train acc: 96.10% | Test loss: 0.9300 | Test acc: 88.40%\n",
      "Epoch: 382 | Train loss: 0.1366 | Train acc: 96.07% | Test loss: 0.9072 | Test acc: 88.56%\n",
      "Epoch: 383 | Train loss: 0.1510 | Train acc: 95.76% | Test loss: 0.9133 | Test acc: 88.15%\n",
      "Epoch: 384 | Train loss: 0.1370 | Train acc: 96.09% | Test loss: 0.8959 | Test acc: 88.29%\n",
      "Epoch: 385 | Train loss: 0.1208 | Train acc: 96.54% | Test loss: 1.1222 | Test acc: 85.41%\n",
      "Epoch: 386 | Train loss: 0.1430 | Train acc: 95.97% | Test loss: 0.8755 | Test acc: 88.60%\n",
      "Epoch: 387 | Train loss: 0.1272 | Train acc: 96.36% | Test loss: 1.0025 | Test acc: 88.28%\n",
      "Epoch: 388 | Train loss: 0.1441 | Train acc: 95.91% | Test loss: 1.0291 | Test acc: 88.46%\n",
      "Epoch: 389 | Train loss: 0.1249 | Train acc: 96.33% | Test loss: 0.9268 | Test acc: 88.91%\n",
      "Epoch: 390 | Train loss: 0.1313 | Train acc: 96.18% | Test loss: 0.9187 | Test acc: 88.74%\n",
      "Epoch: 391 | Train loss: 0.1486 | Train acc: 95.91% | Test loss: 0.9119 | Test acc: 88.85%\n",
      "Epoch: 392 | Train loss: 0.1507 | Train acc: 95.94% | Test loss: 0.9718 | Test acc: 88.38%\n",
      "Epoch: 393 | Train loss: 0.1456 | Train acc: 95.89% | Test loss: 0.8585 | Test acc: 88.56%\n",
      "Epoch: 394 | Train loss: 0.1294 | Train acc: 96.19% | Test loss: 0.9295 | Test acc: 88.53%\n",
      "Epoch: 395 | Train loss: 0.1254 | Train acc: 96.38% | Test loss: 0.9755 | Test acc: 88.64%\n",
      "Epoch: 396 | Train loss: 0.1336 | Train acc: 96.19% | Test loss: 0.9382 | Test acc: 88.73%\n",
      "Epoch: 397 | Train loss: 0.1300 | Train acc: 96.25% | Test loss: 0.9583 | Test acc: 89.08%\n",
      "Epoch: 398 | Train loss: 0.1166 | Train acc: 96.56% | Test loss: 0.9211 | Test acc: 88.81%\n",
      "Epoch: 399 | Train loss: 0.1267 | Train acc: 96.42% | Test loss: 0.9781 | Test acc: 87.85%\n",
      "Epoch: 400 | Train loss: 0.1379 | Train acc: 96.18% | Test loss: 0.9984 | Test acc: 88.60%\n",
      "Epoch: 401 | Train loss: 0.1321 | Train acc: 96.33% | Test loss: 0.9290 | Test acc: 88.12%\n",
      "Epoch: 402 | Train loss: 0.1412 | Train acc: 96.05% | Test loss: 0.9445 | Test acc: 88.15%\n",
      "Epoch: 403 | Train loss: 0.1377 | Train acc: 96.09% | Test loss: 1.0047 | Test acc: 88.71%\n",
      "Epoch: 404 | Train loss: 0.1428 | Train acc: 96.11% | Test loss: 0.9906 | Test acc: 88.87%\n",
      "Epoch: 405 | Train loss: 0.1435 | Train acc: 95.95% | Test loss: 1.0214 | Test acc: 88.83%\n",
      "Epoch: 406 | Train loss: 0.1403 | Train acc: 96.08% | Test loss: 0.8891 | Test acc: 88.50%\n",
      "Epoch: 407 | Train loss: 0.1356 | Train acc: 96.18% | Test loss: 1.0035 | Test acc: 87.74%\n",
      "Epoch: 408 | Train loss: 0.1351 | Train acc: 96.18% | Test loss: 0.8885 | Test acc: 88.59%\n",
      "Epoch: 409 | Train loss: 0.1247 | Train acc: 96.30% | Test loss: 0.9675 | Test acc: 88.48%\n",
      "Epoch: 410 | Train loss: 0.1186 | Train acc: 96.56% | Test loss: 0.9106 | Test acc: 88.55%\n",
      "Epoch: 411 | Train loss: 0.1325 | Train acc: 96.11% | Test loss: 0.8758 | Test acc: 89.00%\n",
      "Epoch: 412 | Train loss: 0.1302 | Train acc: 96.35% | Test loss: 0.9758 | Test acc: 88.68%\n",
      "Epoch: 413 | Train loss: 0.1329 | Train acc: 96.29% | Test loss: 0.9339 | Test acc: 87.28%\n",
      "Epoch: 414 | Train loss: 0.1349 | Train acc: 96.22% | Test loss: 0.9002 | Test acc: 88.25%\n",
      "Epoch: 415 | Train loss: 0.1220 | Train acc: 96.46% | Test loss: 0.9459 | Test acc: 88.52%\n",
      "Epoch: 416 | Train loss: 0.1262 | Train acc: 96.35% | Test loss: 0.9087 | Test acc: 88.65%\n",
      "Epoch: 417 | Train loss: 0.1271 | Train acc: 96.40% | Test loss: 0.9080 | Test acc: 87.91%\n",
      "Epoch: 418 | Train loss: 0.1468 | Train acc: 96.02% | Test loss: 0.9065 | Test acc: 88.16%\n",
      "Epoch: 419 | Train loss: 0.1331 | Train acc: 96.31% | Test loss: 0.9540 | Test acc: 88.71%\n",
      "Epoch: 420 | Train loss: 0.1315 | Train acc: 96.32% | Test loss: 0.9126 | Test acc: 88.72%\n",
      "Epoch: 421 | Train loss: 0.1403 | Train acc: 96.12% | Test loss: 0.9645 | Test acc: 88.58%\n",
      "Epoch: 422 | Train loss: 0.1199 | Train acc: 96.69% | Test loss: 1.1098 | Test acc: 88.73%\n",
      "Epoch: 423 | Train loss: 0.1217 | Train acc: 96.54% | Test loss: 0.9993 | Test acc: 88.73%\n",
      "Epoch: 424 | Train loss: 0.1331 | Train acc: 96.30% | Test loss: 1.0045 | Test acc: 88.36%\n",
      "Epoch: 425 | Train loss: 0.1289 | Train acc: 96.33% | Test loss: 0.9223 | Test acc: 88.61%\n",
      "Epoch: 426 | Train loss: 0.1421 | Train acc: 96.05% | Test loss: 0.9315 | Test acc: 87.96%\n",
      "Epoch: 427 | Train loss: 0.1220 | Train acc: 96.52% | Test loss: 0.9883 | Test acc: 87.77%\n",
      "Epoch: 428 | Train loss: 0.1384 | Train acc: 96.23% | Test loss: 1.0213 | Test acc: 88.48%\n",
      "Epoch: 429 | Train loss: 0.1483 | Train acc: 95.92% | Test loss: 0.9248 | Test acc: 87.82%\n",
      "Epoch: 430 | Train loss: 0.1370 | Train acc: 96.06% | Test loss: 0.9985 | Test acc: 88.30%\n",
      "Epoch: 431 | Train loss: 0.1228 | Train acc: 96.36% | Test loss: 1.0195 | Test acc: 88.54%\n",
      "Epoch: 432 | Train loss: 0.1319 | Train acc: 96.35% | Test loss: 1.0316 | Test acc: 88.40%\n",
      "Epoch: 433 | Train loss: 0.1473 | Train acc: 95.93% | Test loss: 0.9264 | Test acc: 88.39%\n",
      "Epoch: 434 | Train loss: 0.1366 | Train acc: 96.13% | Test loss: 0.9948 | Test acc: 89.03%\n",
      "Epoch: 435 | Train loss: 0.1225 | Train acc: 96.46% | Test loss: 0.9106 | Test acc: 89.13%\n",
      "Epoch: 436 | Train loss: 0.1259 | Train acc: 96.31% | Test loss: 0.9433 | Test acc: 88.48%\n",
      "Epoch: 437 | Train loss: 0.1308 | Train acc: 96.31% | Test loss: 0.9366 | Test acc: 88.70%\n",
      "Epoch: 438 | Train loss: 0.1383 | Train acc: 96.10% | Test loss: 1.0316 | Test acc: 88.24%\n",
      "Epoch: 439 | Train loss: 0.1356 | Train acc: 96.17% | Test loss: 0.9874 | Test acc: 88.31%\n",
      "Epoch: 440 | Train loss: 0.1448 | Train acc: 95.97% | Test loss: 0.9037 | Test acc: 88.30%\n",
      "Epoch: 441 | Train loss: 0.1342 | Train acc: 96.17% | Test loss: 1.0843 | Test acc: 87.25%\n",
      "Epoch: 442 | Train loss: 0.1346 | Train acc: 96.19% | Test loss: 0.8815 | Test acc: 88.69%\n",
      "Epoch: 443 | Train loss: 0.1220 | Train acc: 96.47% | Test loss: 1.0843 | Test acc: 88.72%\n",
      "Epoch: 444 | Train loss: 0.1247 | Train acc: 96.50% | Test loss: 0.9767 | Test acc: 88.37%\n",
      "Epoch: 445 | Train loss: 0.1375 | Train acc: 96.10% | Test loss: 0.9631 | Test acc: 88.04%\n",
      "Epoch: 446 | Train loss: 0.1331 | Train acc: 96.33% | Test loss: 0.9836 | Test acc: 88.07%\n",
      "Epoch: 447 | Train loss: 0.1419 | Train acc: 96.03% | Test loss: 1.0796 | Test acc: 88.64%\n",
      "Epoch: 448 | Train loss: 0.1387 | Train acc: 96.12% | Test loss: 0.9901 | Test acc: 88.66%\n",
      "Epoch: 449 | Train loss: 0.1209 | Train acc: 96.53% | Test loss: 1.0372 | Test acc: 88.11%\n",
      "Epoch: 450 | Train loss: 0.1210 | Train acc: 96.64% | Test loss: 1.1325 | Test acc: 88.82%\n",
      "Epoch: 451 | Train loss: 0.1248 | Train acc: 96.47% | Test loss: 1.0352 | Test acc: 88.51%\n",
      "Epoch: 452 | Train loss: 0.1266 | Train acc: 96.40% | Test loss: 1.0426 | Test acc: 88.79%\n",
      "Epoch: 453 | Train loss: 0.1277 | Train acc: 96.30% | Test loss: 1.0205 | Test acc: 88.77%\n",
      "Epoch: 454 | Train loss: 0.1463 | Train acc: 96.06% | Test loss: 1.0526 | Test acc: 88.06%\n",
      "Epoch: 455 | Train loss: 0.1411 | Train acc: 96.05% | Test loss: 0.9842 | Test acc: 88.67%\n",
      "Epoch: 456 | Train loss: 0.1232 | Train acc: 96.47% | Test loss: 1.1096 | Test acc: 88.55%\n",
      "Epoch: 457 | Train loss: 0.1333 | Train acc: 96.30% | Test loss: 0.9650 | Test acc: 88.16%\n",
      "Epoch: 458 | Train loss: 0.1278 | Train acc: 96.36% | Test loss: 0.9743 | Test acc: 88.50%\n",
      "Epoch: 459 | Train loss: 0.1195 | Train acc: 96.59% | Test loss: 1.0351 | Test acc: 88.60%\n",
      "Epoch: 460 | Train loss: 0.1486 | Train acc: 95.87% | Test loss: 1.0099 | Test acc: 88.59%\n",
      "Epoch: 461 | Train loss: 0.1255 | Train acc: 96.36% | Test loss: 1.0179 | Test acc: 88.61%\n",
      "Epoch: 462 | Train loss: 0.1189 | Train acc: 96.58% | Test loss: 1.0782 | Test acc: 88.02%\n",
      "Epoch: 463 | Train loss: 0.1351 | Train acc: 96.33% | Test loss: 1.0195 | Test acc: 87.86%\n",
      "Epoch: 464 | Train loss: 0.1436 | Train acc: 96.13% | Test loss: 1.0428 | Test acc: 88.50%\n",
      "Epoch: 465 | Train loss: 0.1293 | Train acc: 96.27% | Test loss: 1.0025 | Test acc: 88.76%\n",
      "Epoch: 466 | Train loss: 0.1254 | Train acc: 96.47% | Test loss: 1.0125 | Test acc: 88.57%\n",
      "Epoch: 467 | Train loss: 0.1390 | Train acc: 96.11% | Test loss: 1.0212 | Test acc: 88.54%\n",
      "Epoch: 468 | Train loss: 0.1294 | Train acc: 96.38% | Test loss: 1.0493 | Test acc: 88.97%\n",
      "Epoch: 469 | Train loss: 0.1325 | Train acc: 96.30% | Test loss: 1.0557 | Test acc: 88.90%\n",
      "Epoch: 470 | Train loss: 0.1365 | Train acc: 96.26% | Test loss: 1.0573 | Test acc: 88.18%\n",
      "Epoch: 471 | Train loss: 0.1378 | Train acc: 96.28% | Test loss: 1.0849 | Test acc: 88.16%\n",
      "Epoch: 472 | Train loss: 0.1419 | Train acc: 96.15% | Test loss: 1.1038 | Test acc: 87.93%\n",
      "Epoch: 473 | Train loss: 0.1384 | Train acc: 96.20% | Test loss: 1.0841 | Test acc: 88.58%\n",
      "Epoch: 474 | Train loss: 0.1200 | Train acc: 96.54% | Test loss: 0.9529 | Test acc: 88.48%\n",
      "Epoch: 475 | Train loss: 0.1250 | Train acc: 96.50% | Test loss: 1.0094 | Test acc: 88.29%\n",
      "Epoch: 476 | Train loss: 0.1313 | Train acc: 96.38% | Test loss: 1.0787 | Test acc: 87.85%\n",
      "Epoch: 477 | Train loss: 0.1446 | Train acc: 96.07% | Test loss: 1.0045 | Test acc: 88.08%\n",
      "Epoch: 478 | Train loss: 0.1441 | Train acc: 96.00% | Test loss: 1.0202 | Test acc: 87.70%\n",
      "Epoch: 479 | Train loss: 0.1173 | Train acc: 96.66% | Test loss: 1.0074 | Test acc: 88.87%\n",
      "Epoch: 480 | Train loss: 0.1311 | Train acc: 96.39% | Test loss: 0.9827 | Test acc: 88.64%\n",
      "Epoch: 481 | Train loss: 0.1181 | Train acc: 96.63% | Test loss: 1.0354 | Test acc: 88.46%\n",
      "Epoch: 482 | Train loss: 0.1237 | Train acc: 96.48% | Test loss: 0.9896 | Test acc: 88.38%\n",
      "Epoch: 483 | Train loss: 0.1247 | Train acc: 96.53% | Test loss: 1.0955 | Test acc: 88.92%\n",
      "Epoch: 484 | Train loss: 0.1464 | Train acc: 96.03% | Test loss: 1.0876 | Test acc: 88.18%\n",
      "Epoch: 485 | Train loss: 0.1321 | Train acc: 96.38% | Test loss: 1.0301 | Test acc: 88.41%\n",
      "Epoch: 486 | Train loss: 0.1360 | Train acc: 96.28% | Test loss: 1.0802 | Test acc: 88.59%\n",
      "Epoch: 487 | Train loss: 0.1334 | Train acc: 96.39% | Test loss: 1.0122 | Test acc: 88.49%\n",
      "Epoch: 488 | Train loss: 0.1234 | Train acc: 96.52% | Test loss: 1.1509 | Test acc: 88.62%\n",
      "Epoch: 489 | Train loss: 0.1290 | Train acc: 96.38% | Test loss: 1.1252 | Test acc: 87.70%\n",
      "Epoch: 490 | Train loss: 0.1471 | Train acc: 96.02% | Test loss: 1.0522 | Test acc: 87.94%\n",
      "Epoch: 491 | Train loss: 0.1401 | Train acc: 96.14% | Test loss: 1.0345 | Test acc: 88.25%\n",
      "Epoch: 492 | Train loss: 0.1393 | Train acc: 96.09% | Test loss: 1.1251 | Test acc: 88.30%\n",
      "Epoch: 493 | Train loss: 0.1374 | Train acc: 96.27% | Test loss: 1.0358 | Test acc: 87.82%\n",
      "Epoch: 494 | Train loss: 0.1430 | Train acc: 96.05% | Test loss: 1.0229 | Test acc: 88.18%\n",
      "Epoch: 495 | Train loss: 0.1408 | Train acc: 96.17% | Test loss: 0.9571 | Test acc: 87.95%\n",
      "Epoch: 496 | Train loss: 0.1280 | Train acc: 96.39% | Test loss: 1.1237 | Test acc: 88.83%\n",
      "Epoch: 497 | Train loss: 0.1235 | Train acc: 96.51% | Test loss: 1.1297 | Test acc: 88.27%\n",
      "Epoch: 498 | Train loss: 0.1507 | Train acc: 95.92% | Test loss: 1.0829 | Test acc: 88.89%\n",
      "Epoch: 499 | Train loss: 0.1426 | Train acc: 96.12% | Test loss: 1.0460 | Test acc: 88.41%\n",
      "Epoch: 500 | Train loss: 0.1523 | Train acc: 95.82% | Test loss: 0.9734 | Test acc: 88.23%\n",
      "Epoch: 501 | Train loss: 0.1550 | Train acc: 95.84% | Test loss: 1.0294 | Test acc: 88.58%\n",
      "Epoch: 502 | Train loss: 0.1303 | Train acc: 96.27% | Test loss: 1.0520 | Test acc: 87.72%\n",
      "Epoch: 503 | Train loss: 0.1158 | Train acc: 96.66% | Test loss: 1.0103 | Test acc: 88.96%\n",
      "Epoch: 504 | Train loss: 0.1407 | Train acc: 96.27% | Test loss: 1.0834 | Test acc: 88.37%\n",
      "Epoch: 505 | Train loss: 0.1557 | Train acc: 95.93% | Test loss: 1.0563 | Test acc: 87.87%\n",
      "Epoch: 506 | Train loss: 0.1431 | Train acc: 96.10% | Test loss: 1.0261 | Test acc: 88.56%\n",
      "Epoch: 507 | Train loss: 0.1372 | Train acc: 96.16% | Test loss: 1.0381 | Test acc: 88.49%\n",
      "Epoch: 508 | Train loss: 0.1387 | Train acc: 96.17% | Test loss: 1.0365 | Test acc: 87.93%\n",
      "Epoch: 509 | Train loss: 0.1351 | Train acc: 96.21% | Test loss: 1.0144 | Test acc: 88.93%\n",
      "Epoch: 510 | Train loss: 0.1401 | Train acc: 96.17% | Test loss: 1.0123 | Test acc: 88.56%\n",
      "Epoch: 511 | Train loss: 0.1339 | Train acc: 96.24% | Test loss: 1.0880 | Test acc: 88.07%\n",
      "Epoch: 512 | Train loss: 0.1413 | Train acc: 96.13% | Test loss: 1.0211 | Test acc: 88.83%\n",
      "Epoch: 513 | Train loss: 0.1447 | Train acc: 96.01% | Test loss: 1.0930 | Test acc: 87.89%\n",
      "Epoch: 514 | Train loss: 0.1323 | Train acc: 96.32% | Test loss: 1.0764 | Test acc: 88.99%\n",
      "Epoch: 515 | Train loss: 0.1340 | Train acc: 96.27% | Test loss: 1.0990 | Test acc: 88.75%\n",
      "Epoch: 516 | Train loss: 0.1365 | Train acc: 96.27% | Test loss: 1.0592 | Test acc: 88.71%\n",
      "Epoch: 517 | Train loss: 0.1320 | Train acc: 96.35% | Test loss: 0.9844 | Test acc: 87.69%\n",
      "Epoch: 518 | Train loss: 0.1424 | Train acc: 96.07% | Test loss: 0.9634 | Test acc: 88.11%\n",
      "Epoch: 519 | Train loss: 0.1500 | Train acc: 95.95% | Test loss: 1.0409 | Test acc: 88.06%\n",
      "Epoch: 520 | Train loss: 0.1305 | Train acc: 96.27% | Test loss: 1.1084 | Test acc: 88.78%\n",
      "Epoch: 521 | Train loss: 0.1161 | Train acc: 96.60% | Test loss: 1.0498 | Test acc: 88.63%\n",
      "Epoch: 522 | Train loss: 0.1214 | Train acc: 96.53% | Test loss: 1.0478 | Test acc: 88.64%\n",
      "Epoch: 523 | Train loss: 0.1316 | Train acc: 96.44% | Test loss: 1.1557 | Test acc: 87.82%\n",
      "Epoch: 524 | Train loss: 0.1327 | Train acc: 96.29% | Test loss: 1.0985 | Test acc: 88.67%\n",
      "Epoch: 525 | Train loss: 0.1350 | Train acc: 96.38% | Test loss: 1.0828 | Test acc: 87.26%\n",
      "Epoch: 526 | Train loss: 0.1348 | Train acc: 96.30% | Test loss: 1.0801 | Test acc: 87.49%\n",
      "Epoch: 527 | Train loss: 0.1433 | Train acc: 96.14% | Test loss: 1.2529 | Test acc: 88.18%\n",
      "Epoch: 528 | Train loss: 0.1375 | Train acc: 96.17% | Test loss: 1.1371 | Test acc: 88.37%\n",
      "Epoch: 529 | Train loss: 0.1397 | Train acc: 96.22% | Test loss: 1.0362 | Test acc: 88.58%\n",
      "Epoch: 530 | Train loss: 0.1377 | Train acc: 96.27% | Test loss: 1.1208 | Test acc: 87.84%\n",
      "Epoch: 531 | Train loss: 0.1518 | Train acc: 96.06% | Test loss: 1.0804 | Test acc: 87.65%\n",
      "Epoch: 532 | Train loss: 0.1446 | Train acc: 96.02% | Test loss: 1.0469 | Test acc: 87.98%\n",
      "Epoch: 533 | Train loss: 0.1597 | Train acc: 95.87% | Test loss: 1.0198 | Test acc: 87.99%\n",
      "Epoch: 534 | Train loss: 0.1608 | Train acc: 95.80% | Test loss: 1.0069 | Test acc: 88.11%\n",
      "Epoch: 535 | Train loss: 0.1516 | Train acc: 96.06% | Test loss: 1.0403 | Test acc: 87.98%\n",
      "Epoch: 536 | Train loss: 0.1517 | Train acc: 96.00% | Test loss: 0.9560 | Test acc: 89.18%\n",
      "Epoch: 537 | Train loss: 0.1196 | Train acc: 96.44% | Test loss: 1.0154 | Test acc: 88.84%\n",
      "Epoch: 538 | Train loss: 0.1298 | Train acc: 96.34% | Test loss: 1.1222 | Test acc: 88.41%\n",
      "Epoch: 539 | Train loss: 0.1472 | Train acc: 96.07% | Test loss: 1.0637 | Test acc: 87.95%\n",
      "Epoch: 540 | Train loss: 0.1350 | Train acc: 96.27% | Test loss: 0.9895 | Test acc: 88.45%\n",
      "Epoch: 541 | Train loss: 0.1401 | Train acc: 96.20% | Test loss: 1.1970 | Test acc: 87.80%\n",
      "Epoch: 542 | Train loss: 0.1521 | Train acc: 95.97% | Test loss: 1.0972 | Test acc: 87.25%\n",
      "Epoch: 543 | Train loss: 0.1673 | Train acc: 95.60% | Test loss: 1.1713 | Test acc: 88.67%\n",
      "Epoch: 544 | Train loss: 0.1533 | Train acc: 95.84% | Test loss: 1.0712 | Test acc: 88.05%\n",
      "Epoch: 545 | Train loss: 0.1351 | Train acc: 96.11% | Test loss: 1.0684 | Test acc: 88.11%\n",
      "Epoch: 546 | Train loss: 0.1127 | Train acc: 96.72% | Test loss: 1.1370 | Test acc: 88.76%\n",
      "Epoch: 547 | Train loss: 0.1313 | Train acc: 96.40% | Test loss: 1.0777 | Test acc: 88.13%\n",
      "Epoch: 548 | Train loss: 0.1566 | Train acc: 95.88% | Test loss: 1.0161 | Test acc: 88.32%\n",
      "Epoch: 549 | Train loss: 0.1492 | Train acc: 96.00% | Test loss: 1.0090 | Test acc: 87.79%\n",
      "Epoch: 550 | Train loss: 0.1318 | Train acc: 96.28% | Test loss: 1.0690 | Test acc: 87.75%\n",
      "Epoch: 551 | Train loss: 0.1411 | Train acc: 96.22% | Test loss: 0.9937 | Test acc: 88.85%\n",
      "Epoch: 552 | Train loss: 0.1378 | Train acc: 96.16% | Test loss: 1.1466 | Test acc: 87.16%\n",
      "Epoch: 553 | Train loss: 0.1307 | Train acc: 96.39% | Test loss: 1.0776 | Test acc: 88.14%\n",
      "Epoch: 554 | Train loss: 0.1497 | Train acc: 96.03% | Test loss: 1.0905 | Test acc: 88.72%\n",
      "Epoch: 555 | Train loss: 0.1363 | Train acc: 96.29% | Test loss: 1.0256 | Test acc: 88.58%\n",
      "Epoch: 556 | Train loss: 0.1182 | Train acc: 96.61% | Test loss: 1.0545 | Test acc: 88.75%\n",
      "Epoch: 557 | Train loss: 0.1424 | Train acc: 96.14% | Test loss: 1.0996 | Test acc: 88.93%\n",
      "Epoch: 558 | Train loss: 0.1567 | Train acc: 95.85% | Test loss: 1.0181 | Test acc: 88.66%\n",
      "Epoch: 559 | Train loss: 0.1354 | Train acc: 96.32% | Test loss: 1.0782 | Test acc: 88.56%\n",
      "Epoch: 560 | Train loss: 0.1361 | Train acc: 96.23% | Test loss: 1.1562 | Test acc: 88.11%\n",
      "Epoch: 561 | Train loss: 0.1306 | Train acc: 96.41% | Test loss: 1.1217 | Test acc: 87.93%\n",
      "Epoch: 562 | Train loss: 0.1452 | Train acc: 96.05% | Test loss: 1.0218 | Test acc: 87.69%\n",
      "Epoch: 563 | Train loss: 0.1318 | Train acc: 96.33% | Test loss: 1.1165 | Test acc: 88.42%\n",
      "Epoch: 564 | Train loss: 0.1474 | Train acc: 96.10% | Test loss: 1.0597 | Test acc: 88.16%\n",
      "Epoch: 565 | Train loss: 0.1401 | Train acc: 96.15% | Test loss: 1.1191 | Test acc: 88.32%\n",
      "Epoch: 566 | Train loss: 0.1623 | Train acc: 95.81% | Test loss: 1.1062 | Test acc: 88.00%\n",
      "Epoch: 567 | Train loss: 0.1249 | Train acc: 96.41% | Test loss: 1.0354 | Test acc: 88.23%\n",
      "Epoch: 568 | Train loss: 0.1325 | Train acc: 96.30% | Test loss: 1.0666 | Test acc: 88.46%\n",
      "Epoch: 569 | Train loss: 0.1425 | Train acc: 96.15% | Test loss: 1.0904 | Test acc: 88.52%\n",
      "Epoch: 570 | Train loss: 0.1683 | Train acc: 95.67% | Test loss: 1.1833 | Test acc: 87.25%\n",
      "Epoch: 571 | Train loss: 0.1491 | Train acc: 95.88% | Test loss: 1.0512 | Test acc: 87.92%\n",
      "Epoch: 572 | Train loss: 0.1443 | Train acc: 96.14% | Test loss: 1.0751 | Test acc: 88.75%\n",
      "Epoch: 573 | Train loss: 0.1589 | Train acc: 95.77% | Test loss: 1.0643 | Test acc: 87.80%\n",
      "Epoch: 574 | Train loss: 0.1436 | Train acc: 96.09% | Test loss: 1.0091 | Test acc: 88.54%\n",
      "Epoch: 575 | Train loss: 0.1378 | Train acc: 96.23% | Test loss: 1.0493 | Test acc: 88.53%\n",
      "Epoch: 576 | Train loss: 0.1262 | Train acc: 96.43% | Test loss: 1.0836 | Test acc: 88.46%\n",
      "Epoch: 577 | Train loss: 0.1401 | Train acc: 96.24% | Test loss: 1.0836 | Test acc: 88.21%\n",
      "Epoch: 578 | Train loss: 0.1581 | Train acc: 95.90% | Test loss: 1.1347 | Test acc: 87.53%\n",
      "Epoch: 579 | Train loss: 0.1452 | Train acc: 96.05% | Test loss: 1.0596 | Test acc: 88.65%\n",
      "Epoch: 580 | Train loss: 0.1548 | Train acc: 96.05% | Test loss: 1.1145 | Test acc: 88.34%\n",
      "Epoch: 581 | Train loss: 0.1415 | Train acc: 96.25% | Test loss: 1.1227 | Test acc: 88.48%\n",
      "Epoch: 582 | Train loss: 0.1564 | Train acc: 95.80% | Test loss: 1.0937 | Test acc: 88.14%\n",
      "Epoch: 583 | Train loss: 0.1528 | Train acc: 95.90% | Test loss: 0.9829 | Test acc: 87.78%\n",
      "Epoch: 584 | Train loss: 0.1254 | Train acc: 96.52% | Test loss: 1.0630 | Test acc: 88.51%\n",
      "Epoch: 585 | Train loss: 0.1282 | Train acc: 96.36% | Test loss: 1.0314 | Test acc: 88.29%\n",
      "Epoch: 586 | Train loss: 0.1386 | Train acc: 96.35% | Test loss: 1.0779 | Test acc: 87.87%\n",
      "Epoch: 587 | Train loss: 0.1417 | Train acc: 96.23% | Test loss: 1.0568 | Test acc: 88.83%\n",
      "Epoch: 588 | Train loss: 0.1424 | Train acc: 96.17% | Test loss: 1.0670 | Test acc: 88.09%\n",
      "Epoch: 589 | Train loss: 0.1553 | Train acc: 95.94% | Test loss: 1.1642 | Test acc: 88.14%\n",
      "Epoch: 590 | Train loss: 0.1602 | Train acc: 95.83% | Test loss: 1.0352 | Test acc: 88.18%\n",
      "Epoch: 591 | Train loss: 0.1511 | Train acc: 95.94% | Test loss: 1.0294 | Test acc: 88.11%\n",
      "Epoch: 592 | Train loss: 0.1607 | Train acc: 95.70% | Test loss: 1.0924 | Test acc: 88.56%\n",
      "Epoch: 593 | Train loss: 0.1585 | Train acc: 95.82% | Test loss: 1.0826 | Test acc: 88.04%\n",
      "Epoch: 594 | Train loss: 0.1357 | Train acc: 96.25% | Test loss: 1.0171 | Test acc: 88.22%\n",
      "Epoch: 595 | Train loss: 0.1383 | Train acc: 96.31% | Test loss: 1.1168 | Test acc: 88.03%\n",
      "Epoch: 596 | Train loss: 0.1468 | Train acc: 95.97% | Test loss: 1.0947 | Test acc: 88.13%\n",
      "Epoch: 597 | Train loss: 0.1578 | Train acc: 95.88% | Test loss: 1.0509 | Test acc: 88.11%\n",
      "Epoch: 598 | Train loss: 0.1338 | Train acc: 96.26% | Test loss: 1.0562 | Test acc: 88.52%\n",
      "Epoch: 599 | Train loss: 0.1318 | Train acc: 96.34% | Test loss: 1.0957 | Test acc: 87.44%\n",
      "Epoch: 600 | Train loss: 0.1417 | Train acc: 96.12% | Test loss: 1.1240 | Test acc: 88.33%\n",
      "Epoch: 601 | Train loss: 0.1195 | Train acc: 96.69% | Test loss: 1.1980 | Test acc: 88.08%\n",
      "Epoch: 602 | Train loss: 0.1362 | Train acc: 96.33% | Test loss: 1.1340 | Test acc: 87.71%\n",
      "Epoch: 603 | Train loss: 0.1332 | Train acc: 96.42% | Test loss: 1.1620 | Test acc: 88.65%\n",
      "Epoch: 604 | Train loss: 0.1374 | Train acc: 96.31% | Test loss: 1.0688 | Test acc: 88.25%\n",
      "Epoch: 605 | Train loss: 0.1498 | Train acc: 95.98% | Test loss: 1.1154 | Test acc: 89.44%\n",
      "Epoch: 606 | Train loss: 0.1420 | Train acc: 96.21% | Test loss: 1.0923 | Test acc: 88.30%\n",
      "Epoch: 607 | Train loss: 0.1361 | Train acc: 96.28% | Test loss: 1.0269 | Test acc: 87.90%\n",
      "Epoch: 608 | Train loss: 0.1355 | Train acc: 96.34% | Test loss: 1.0603 | Test acc: 88.11%\n",
      "Epoch: 609 | Train loss: 0.1164 | Train acc: 96.61% | Test loss: 1.0482 | Test acc: 88.45%\n",
      "Epoch: 610 | Train loss: 0.1223 | Train acc: 96.69% | Test loss: 1.0727 | Test acc: 88.34%\n",
      "Epoch: 611 | Train loss: 0.1414 | Train acc: 96.18% | Test loss: 1.1730 | Test acc: 87.24%\n",
      "Epoch: 612 | Train loss: 0.1551 | Train acc: 95.95% | Test loss: 1.0596 | Test acc: 88.15%\n",
      "Epoch: 613 | Train loss: 0.1517 | Train acc: 95.95% | Test loss: 1.0669 | Test acc: 87.13%\n",
      "Epoch: 614 | Train loss: 0.1712 | Train acc: 95.63% | Test loss: 1.1173 | Test acc: 88.12%\n",
      "Epoch: 615 | Train loss: 0.1510 | Train acc: 95.88% | Test loss: 1.1137 | Test acc: 88.11%\n",
      "Epoch: 616 | Train loss: 0.1289 | Train acc: 96.40% | Test loss: 1.0965 | Test acc: 87.98%\n",
      "Epoch: 617 | Train loss: 0.1525 | Train acc: 95.92% | Test loss: 1.0876 | Test acc: 88.41%\n",
      "Epoch: 618 | Train loss: 0.1272 | Train acc: 96.44% | Test loss: 1.1226 | Test acc: 88.52%\n",
      "Epoch: 619 | Train loss: 0.1325 | Train acc: 96.37% | Test loss: 1.1339 | Test acc: 88.13%\n",
      "Epoch: 620 | Train loss: 0.1758 | Train acc: 95.63% | Test loss: 1.1507 | Test acc: 87.53%\n",
      "Epoch: 621 | Train loss: 0.1606 | Train acc: 95.80% | Test loss: 1.1118 | Test acc: 88.57%\n",
      "Epoch: 622 | Train loss: 0.1343 | Train acc: 96.34% | Test loss: 1.1451 | Test acc: 88.85%\n",
      "Epoch: 623 | Train loss: 0.1311 | Train acc: 96.41% | Test loss: 1.0837 | Test acc: 87.67%\n",
      "Epoch: 624 | Train loss: 0.1331 | Train acc: 96.45% | Test loss: 1.1894 | Test acc: 88.07%\n",
      "Epoch: 625 | Train loss: 0.1769 | Train acc: 95.54% | Test loss: 1.1475 | Test acc: 87.66%\n",
      "Epoch: 626 | Train loss: 0.1489 | Train acc: 96.01% | Test loss: 1.1082 | Test acc: 88.29%\n",
      "Epoch: 627 | Train loss: 0.1474 | Train acc: 96.06% | Test loss: 1.1341 | Test acc: 88.35%\n",
      "Epoch: 628 | Train loss: 0.1287 | Train acc: 96.50% | Test loss: 1.0819 | Test acc: 88.53%\n",
      "Epoch: 629 | Train loss: 0.1192 | Train acc: 96.75% | Test loss: 1.1072 | Test acc: 88.06%\n",
      "Epoch: 630 | Train loss: 0.1529 | Train acc: 95.96% | Test loss: 1.1944 | Test acc: 88.31%\n",
      "Epoch: 631 | Train loss: 0.1474 | Train acc: 96.13% | Test loss: 1.0426 | Test acc: 87.48%\n",
      "Epoch: 632 | Train loss: 0.1425 | Train acc: 96.16% | Test loss: 1.0660 | Test acc: 88.86%\n",
      "Epoch: 633 | Train loss: 0.1450 | Train acc: 96.16% | Test loss: 1.1837 | Test acc: 88.29%\n",
      "Epoch: 634 | Train loss: 0.1621 | Train acc: 95.80% | Test loss: 1.2241 | Test acc: 88.11%\n",
      "Epoch: 635 | Train loss: 0.1489 | Train acc: 96.13% | Test loss: 1.0939 | Test acc: 88.32%\n",
      "Epoch: 636 | Train loss: 0.1401 | Train acc: 96.23% | Test loss: 1.0713 | Test acc: 87.81%\n",
      "Epoch: 637 | Train loss: 0.1422 | Train acc: 96.20% | Test loss: 1.0398 | Test acc: 88.44%\n",
      "Epoch: 638 | Train loss: 0.1432 | Train acc: 96.21% | Test loss: 1.0413 | Test acc: 87.66%\n",
      "Epoch: 639 | Train loss: 0.1444 | Train acc: 96.28% | Test loss: 1.1420 | Test acc: 88.42%\n",
      "Epoch: 640 | Train loss: 0.1443 | Train acc: 96.19% | Test loss: 1.0407 | Test acc: 87.31%\n",
      "Epoch: 641 | Train loss: 0.1724 | Train acc: 95.79% | Test loss: 1.0640 | Test acc: 88.00%\n",
      "Epoch: 642 | Train loss: 0.1525 | Train acc: 96.00% | Test loss: 1.0299 | Test acc: 88.16%\n",
      "Epoch: 643 | Train loss: 0.1217 | Train acc: 96.58% | Test loss: 1.1153 | Test acc: 88.61%\n",
      "Epoch: 644 | Train loss: 0.1272 | Train acc: 96.53% | Test loss: 1.1131 | Test acc: 88.14%\n",
      "Epoch: 645 | Train loss: 0.1235 | Train acc: 96.53% | Test loss: 1.1528 | Test acc: 87.99%\n",
      "Epoch: 646 | Train loss: 0.1324 | Train acc: 96.45% | Test loss: 1.3262 | Test acc: 87.28%\n",
      "Epoch: 647 | Train loss: 0.1451 | Train acc: 96.14% | Test loss: 1.1391 | Test acc: 88.02%\n",
      "Epoch: 648 | Train loss: 0.1235 | Train acc: 96.42% | Test loss: 1.1650 | Test acc: 86.90%\n",
      "Epoch: 649 | Train loss: 0.1579 | Train acc: 95.98% | Test loss: 1.1719 | Test acc: 87.71%\n",
      "Epoch: 650 | Train loss: 0.1495 | Train acc: 96.12% | Test loss: 1.2584 | Test acc: 88.59%\n",
      "Epoch: 651 | Train loss: 0.1502 | Train acc: 95.98% | Test loss: 1.0539 | Test acc: 88.19%\n",
      "Epoch: 652 | Train loss: 0.1441 | Train acc: 96.23% | Test loss: 1.1725 | Test acc: 87.05%\n",
      "Epoch: 653 | Train loss: 0.1457 | Train acc: 96.19% | Test loss: 1.1754 | Test acc: 88.72%\n",
      "Epoch: 654 | Train loss: 0.1475 | Train acc: 96.03% | Test loss: 1.1123 | Test acc: 88.07%\n",
      "Epoch: 655 | Train loss: 0.1504 | Train acc: 96.13% | Test loss: 1.0940 | Test acc: 88.11%\n",
      "Epoch: 656 | Train loss: 0.1530 | Train acc: 95.92% | Test loss: 1.0685 | Test acc: 87.73%\n",
      "Epoch: 657 | Train loss: 0.1462 | Train acc: 96.08% | Test loss: 1.1159 | Test acc: 88.78%\n",
      "Epoch: 658 | Train loss: 0.1552 | Train acc: 95.94% | Test loss: 1.1752 | Test acc: 87.93%\n",
      "Epoch: 659 | Train loss: 0.1585 | Train acc: 95.83% | Test loss: 1.1326 | Test acc: 87.49%\n",
      "Epoch: 660 | Train loss: 0.1409 | Train acc: 96.16% | Test loss: 1.1474 | Test acc: 88.02%\n",
      "Epoch: 661 | Train loss: 0.1292 | Train acc: 96.39% | Test loss: 1.1581 | Test acc: 88.18%\n",
      "Epoch: 662 | Train loss: 0.1404 | Train acc: 96.40% | Test loss: 1.1540 | Test acc: 87.94%\n",
      "Epoch: 663 | Train loss: 0.1292 | Train acc: 96.45% | Test loss: 1.2624 | Test acc: 87.84%\n",
      "Epoch: 664 | Train loss: 0.1202 | Train acc: 96.67% | Test loss: 1.2654 | Test acc: 88.06%\n",
      "Epoch: 665 | Train loss: 0.1327 | Train acc: 96.39% | Test loss: 1.1475 | Test acc: 87.84%\n",
      "Epoch: 666 | Train loss: 0.1408 | Train acc: 96.40% | Test loss: 1.1581 | Test acc: 88.80%\n",
      "Epoch: 667 | Train loss: 0.1338 | Train acc: 96.38% | Test loss: 1.1520 | Test acc: 88.03%\n",
      "Epoch: 668 | Train loss: 0.1373 | Train acc: 96.37% | Test loss: 1.2794 | Test acc: 87.87%\n",
      "Epoch: 669 | Train loss: 0.1641 | Train acc: 95.89% | Test loss: 1.1358 | Test acc: 87.87%\n",
      "Epoch: 670 | Train loss: 0.1905 | Train acc: 95.42% | Test loss: 1.0356 | Test acc: 88.01%\n",
      "Epoch: 671 | Train loss: 0.1511 | Train acc: 95.89% | Test loss: 1.1446 | Test acc: 88.77%\n",
      "Epoch: 672 | Train loss: 0.1435 | Train acc: 96.16% | Test loss: 1.0725 | Test acc: 87.96%\n",
      "Epoch: 673 | Train loss: 0.1584 | Train acc: 95.85% | Test loss: 1.0955 | Test acc: 87.75%\n",
      "Epoch: 674 | Train loss: 0.1463 | Train acc: 96.03% | Test loss: 1.0895 | Test acc: 88.24%\n",
      "Epoch: 675 | Train loss: 0.1450 | Train acc: 96.12% | Test loss: 1.1157 | Test acc: 88.51%\n",
      "Epoch: 676 | Train loss: 0.1548 | Train acc: 96.05% | Test loss: 1.0807 | Test acc: 87.72%\n",
      "Epoch: 677 | Train loss: 0.1565 | Train acc: 95.90% | Test loss: 1.1940 | Test acc: 88.39%\n",
      "Epoch: 678 | Train loss: 0.1563 | Train acc: 95.92% | Test loss: 1.1110 | Test acc: 87.63%\n",
      "Epoch: 679 | Train loss: 0.1598 | Train acc: 95.73% | Test loss: 1.1814 | Test acc: 88.45%\n",
      "Epoch: 680 | Train loss: 0.1426 | Train acc: 96.23% | Test loss: 1.1528 | Test acc: 87.49%\n",
      "Epoch: 681 | Train loss: 0.1413 | Train acc: 96.25% | Test loss: 1.1061 | Test acc: 88.35%\n",
      "Epoch: 682 | Train loss: 0.1393 | Train acc: 96.26% | Test loss: 1.0710 | Test acc: 87.82%\n",
      "Epoch: 683 | Train loss: 0.1327 | Train acc: 96.46% | Test loss: 1.0994 | Test acc: 87.94%\n",
      "Epoch: 684 | Train loss: 0.1289 | Train acc: 96.51% | Test loss: 1.0780 | Test acc: 87.60%\n",
      "Epoch: 685 | Train loss: 0.1506 | Train acc: 96.01% | Test loss: 1.0717 | Test acc: 88.07%\n",
      "Epoch: 686 | Train loss: 0.1424 | Train acc: 96.20% | Test loss: 1.0631 | Test acc: 88.46%\n",
      "Epoch: 687 | Train loss: 0.1335 | Train acc: 96.37% | Test loss: 1.3005 | Test acc: 88.39%\n",
      "Epoch: 688 | Train loss: 0.1575 | Train acc: 95.99% | Test loss: 1.1371 | Test acc: 87.59%\n",
      "Epoch: 689 | Train loss: 0.1521 | Train acc: 96.02% | Test loss: 1.2310 | Test acc: 88.33%\n",
      "Epoch: 690 | Train loss: 0.1542 | Train acc: 96.01% | Test loss: 1.1947 | Test acc: 88.04%\n",
      "Epoch: 691 | Train loss: 0.1674 | Train acc: 95.83% | Test loss: 1.0899 | Test acc: 87.06%\n",
      "Epoch: 692 | Train loss: 0.1859 | Train acc: 95.31% | Test loss: 1.0722 | Test acc: 87.92%\n",
      "Epoch: 693 | Train loss: 0.1608 | Train acc: 95.70% | Test loss: 1.1446 | Test acc: 87.96%\n",
      "Epoch: 694 | Train loss: 0.1313 | Train acc: 96.41% | Test loss: 1.1860 | Test acc: 87.90%\n",
      "Epoch: 695 | Train loss: 0.1541 | Train acc: 95.98% | Test loss: 1.0426 | Test acc: 87.98%\n",
      "Epoch: 696 | Train loss: 0.1358 | Train acc: 96.32% | Test loss: 1.1147 | Test acc: 87.62%\n",
      "Epoch: 697 | Train loss: 0.1359 | Train acc: 96.42% | Test loss: 1.1101 | Test acc: 87.94%\n",
      "Epoch: 698 | Train loss: 0.1410 | Train acc: 96.20% | Test loss: 1.2030 | Test acc: 87.90%\n",
      "Epoch: 699 | Train loss: 0.1312 | Train acc: 96.42% | Test loss: 1.1377 | Test acc: 88.34%\n",
      "Epoch: 700 | Train loss: 0.1594 | Train acc: 96.00% | Test loss: 1.2526 | Test acc: 87.24%\n",
      "Epoch: 701 | Train loss: 0.1411 | Train acc: 96.28% | Test loss: 1.1305 | Test acc: 87.64%\n",
      "Epoch: 702 | Train loss: 0.1497 | Train acc: 96.11% | Test loss: 1.2939 | Test acc: 87.03%\n",
      "Epoch: 703 | Train loss: 0.1578 | Train acc: 95.94% | Test loss: 1.1648 | Test acc: 87.30%\n",
      "Epoch: 704 | Train loss: 0.1333 | Train acc: 96.35% | Test loss: 1.2151 | Test acc: 88.13%\n",
      "Epoch: 705 | Train loss: 0.1502 | Train acc: 96.02% | Test loss: 1.1927 | Test acc: 87.51%\n",
      "Epoch: 706 | Train loss: 0.1566 | Train acc: 95.93% | Test loss: 1.1078 | Test acc: 88.65%\n",
      "Epoch: 707 | Train loss: 0.1600 | Train acc: 95.84% | Test loss: 1.1001 | Test acc: 87.98%\n",
      "Epoch: 708 | Train loss: 0.1591 | Train acc: 95.74% | Test loss: 1.0792 | Test acc: 88.04%\n",
      "Epoch: 709 | Train loss: 0.1510 | Train acc: 95.95% | Test loss: 1.0797 | Test acc: 88.19%\n",
      "Epoch: 710 | Train loss: 0.1396 | Train acc: 96.20% | Test loss: 1.1272 | Test acc: 88.16%\n",
      "Epoch: 711 | Train loss: 0.1587 | Train acc: 95.91% | Test loss: 1.1007 | Test acc: 88.29%\n",
      "Epoch: 712 | Train loss: 0.1464 | Train acc: 96.14% | Test loss: 1.1884 | Test acc: 88.61%\n",
      "Epoch: 713 | Train loss: 0.1414 | Train acc: 96.31% | Test loss: 1.2003 | Test acc: 88.38%\n",
      "Epoch: 714 | Train loss: 0.1444 | Train acc: 96.17% | Test loss: 1.1967 | Test acc: 87.59%\n",
      "Epoch: 715 | Train loss: 0.1538 | Train acc: 95.96% | Test loss: 1.1033 | Test acc: 88.05%\n",
      "Epoch: 716 | Train loss: 0.1494 | Train acc: 96.10% | Test loss: 1.1163 | Test acc: 88.07%\n",
      "Epoch: 717 | Train loss: 0.1597 | Train acc: 95.98% | Test loss: 1.1637 | Test acc: 87.15%\n",
      "Epoch: 718 | Train loss: 0.1152 | Train acc: 96.72% | Test loss: 1.1290 | Test acc: 88.28%\n",
      "Epoch: 719 | Train loss: 0.1139 | Train acc: 96.85% | Test loss: 1.1820 | Test acc: 88.05%\n",
      "Epoch: 720 | Train loss: 0.1434 | Train acc: 96.33% | Test loss: 1.1818 | Test acc: 88.07%\n",
      "Epoch: 721 | Train loss: 0.1336 | Train acc: 96.45% | Test loss: 1.1855 | Test acc: 88.39%\n",
      "Epoch: 722 | Train loss: 0.1256 | Train acc: 96.55% | Test loss: 1.1109 | Test acc: 87.34%\n",
      "Epoch: 723 | Train loss: 0.1315 | Train acc: 96.53% | Test loss: 1.2848 | Test acc: 88.49%\n",
      "Epoch: 724 | Train loss: 0.1319 | Train acc: 96.50% | Test loss: 1.2245 | Test acc: 87.99%\n",
      "Epoch: 725 | Train loss: 0.1543 | Train acc: 96.04% | Test loss: 1.2646 | Test acc: 87.63%\n",
      "Epoch: 726 | Train loss: 0.1370 | Train acc: 96.38% | Test loss: 1.1868 | Test acc: 88.97%\n",
      "Epoch: 727 | Train loss: 0.1550 | Train acc: 96.11% | Test loss: 1.1674 | Test acc: 87.88%\n",
      "Epoch: 728 | Train loss: 0.1652 | Train acc: 95.88% | Test loss: 1.1662 | Test acc: 88.22%\n",
      "Epoch: 729 | Train loss: 0.1669 | Train acc: 95.84% | Test loss: 1.0833 | Test acc: 88.47%\n",
      "Epoch: 730 | Train loss: 0.1399 | Train acc: 96.31% | Test loss: 1.1205 | Test acc: 88.72%\n",
      "Epoch: 731 | Train loss: 0.1554 | Train acc: 96.02% | Test loss: 1.1467 | Test acc: 88.67%\n",
      "Epoch: 732 | Train loss: 0.1543 | Train acc: 96.01% | Test loss: 1.2149 | Test acc: 88.06%\n",
      "Epoch: 733 | Train loss: 0.1576 | Train acc: 95.96% | Test loss: 1.1631 | Test acc: 88.29%\n",
      "Epoch: 734 | Train loss: 0.1240 | Train acc: 96.54% | Test loss: 1.1600 | Test acc: 88.17%\n",
      "Epoch: 735 | Train loss: 0.1290 | Train acc: 96.55% | Test loss: 1.1649 | Test acc: 88.52%\n",
      "Epoch: 736 | Train loss: 0.1515 | Train acc: 96.04% | Test loss: 1.1994 | Test acc: 87.97%\n",
      "Epoch: 737 | Train loss: 0.1547 | Train acc: 96.11% | Test loss: 1.1312 | Test acc: 87.82%\n",
      "Epoch: 738 | Train loss: 0.1826 | Train acc: 95.45% | Test loss: 1.0260 | Test acc: 87.66%\n",
      "Epoch: 739 | Train loss: 0.1447 | Train acc: 96.25% | Test loss: 1.3005 | Test acc: 88.38%\n",
      "Epoch: 740 | Train loss: 0.1584 | Train acc: 96.00% | Test loss: 1.2765 | Test acc: 87.62%\n",
      "Epoch: 741 | Train loss: 0.1495 | Train acc: 96.04% | Test loss: 1.1842 | Test acc: 88.27%\n",
      "Epoch: 742 | Train loss: 0.1517 | Train acc: 96.06% | Test loss: 1.1535 | Test acc: 87.85%\n",
      "Epoch: 743 | Train loss: 0.1406 | Train acc: 96.28% | Test loss: 1.0879 | Test acc: 88.12%\n",
      "Epoch: 744 | Train loss: 0.1399 | Train acc: 96.25% | Test loss: 1.1040 | Test acc: 87.77%\n",
      "Epoch: 745 | Train loss: 0.1585 | Train acc: 95.97% | Test loss: 1.1436 | Test acc: 87.77%\n",
      "Epoch: 746 | Train loss: 0.1520 | Train acc: 96.10% | Test loss: 1.1278 | Test acc: 88.55%\n",
      "Epoch: 747 | Train loss: 0.1352 | Train acc: 96.36% | Test loss: 1.1608 | Test acc: 88.40%\n",
      "Epoch: 748 | Train loss: 0.1503 | Train acc: 96.08% | Test loss: 1.0925 | Test acc: 87.66%\n",
      "Epoch: 749 | Train loss: 0.1343 | Train acc: 96.36% | Test loss: 1.1037 | Test acc: 88.07%\n",
      "Epoch: 750 | Train loss: 0.1410 | Train acc: 96.25% | Test loss: 1.2663 | Test acc: 86.54%\n",
      "Epoch: 751 | Train loss: 0.1455 | Train acc: 96.16% | Test loss: 1.1782 | Test acc: 87.52%\n",
      "Epoch: 752 | Train loss: 0.1361 | Train acc: 96.30% | Test loss: 1.0941 | Test acc: 88.28%\n",
      "Epoch: 753 | Train loss: 0.1212 | Train acc: 96.60% | Test loss: 1.1937 | Test acc: 88.04%\n",
      "Epoch: 754 | Train loss: 0.1555 | Train acc: 96.12% | Test loss: 1.2113 | Test acc: 87.85%\n",
      "Epoch: 755 | Train loss: 0.1690 | Train acc: 95.75% | Test loss: 1.2633 | Test acc: 88.75%\n",
      "Epoch: 756 | Train loss: 0.1625 | Train acc: 95.87% | Test loss: 1.1453 | Test acc: 87.82%\n",
      "Epoch: 757 | Train loss: 0.1498 | Train acc: 96.03% | Test loss: 1.3027 | Test acc: 87.98%\n",
      "Epoch: 758 | Train loss: 0.1496 | Train acc: 96.19% | Test loss: 1.1957 | Test acc: 88.59%\n",
      "Epoch: 759 | Train loss: 0.1437 | Train acc: 96.24% | Test loss: 1.1536 | Test acc: 88.27%\n",
      "Epoch: 760 | Train loss: 0.1582 | Train acc: 95.91% | Test loss: 1.2083 | Test acc: 87.98%\n",
      "Epoch: 761 | Train loss: 0.1652 | Train acc: 95.85% | Test loss: 1.2190 | Test acc: 88.05%\n",
      "Epoch: 762 | Train loss: 0.1750 | Train acc: 95.70% | Test loss: 1.2745 | Test acc: 87.35%\n",
      "Epoch: 763 | Train loss: 0.1698 | Train acc: 95.61% | Test loss: 1.1016 | Test acc: 87.91%\n",
      "Epoch: 764 | Train loss: 0.1503 | Train acc: 95.99% | Test loss: 1.2921 | Test acc: 87.89%\n",
      "Epoch: 765 | Train loss: 0.1372 | Train acc: 96.44% | Test loss: 1.1629 | Test acc: 88.37%\n",
      "Epoch: 766 | Train loss: 0.1371 | Train acc: 96.39% | Test loss: 1.1839 | Test acc: 88.22%\n",
      "Epoch: 767 | Train loss: 0.1504 | Train acc: 96.07% | Test loss: 1.2042 | Test acc: 88.12%\n",
      "Epoch: 768 | Train loss: 0.1314 | Train acc: 96.47% | Test loss: 1.2235 | Test acc: 87.25%\n",
      "Epoch: 769 | Train loss: 0.1480 | Train acc: 96.20% | Test loss: 1.1217 | Test acc: 88.35%\n",
      "Epoch: 770 | Train loss: 0.1627 | Train acc: 95.86% | Test loss: 1.1381 | Test acc: 88.29%\n",
      "Epoch: 771 | Train loss: 0.1762 | Train acc: 95.62% | Test loss: 1.0379 | Test acc: 88.13%\n",
      "Epoch: 772 | Train loss: 0.1610 | Train acc: 95.83% | Test loss: 1.1203 | Test acc: 87.70%\n",
      "Epoch: 773 | Train loss: 0.1634 | Train acc: 95.73% | Test loss: 1.1871 | Test acc: 87.89%\n",
      "Epoch: 774 | Train loss: 0.1645 | Train acc: 95.87% | Test loss: 1.2819 | Test acc: 87.89%\n",
      "Epoch: 775 | Train loss: 0.1803 | Train acc: 95.57% | Test loss: 1.1936 | Test acc: 88.01%\n",
      "Epoch: 776 | Train loss: 0.1525 | Train acc: 96.05% | Test loss: 1.1604 | Test acc: 88.36%\n",
      "Epoch: 777 | Train loss: 0.1436 | Train acc: 96.18% | Test loss: 1.1283 | Test acc: 88.17%\n",
      "Epoch: 778 | Train loss: 0.1219 | Train acc: 96.71% | Test loss: 1.2169 | Test acc: 87.97%\n",
      "Epoch: 779 | Train loss: 0.1496 | Train acc: 96.25% | Test loss: 1.2199 | Test acc: 87.72%\n",
      "Epoch: 780 | Train loss: 0.1647 | Train acc: 95.84% | Test loss: 1.2149 | Test acc: 86.75%\n",
      "Epoch: 781 | Train loss: 0.1603 | Train acc: 95.92% | Test loss: 1.2009 | Test acc: 87.51%\n",
      "Epoch: 782 | Train loss: 0.1532 | Train acc: 96.10% | Test loss: 1.2774 | Test acc: 87.93%\n",
      "Epoch: 783 | Train loss: 0.1587 | Train acc: 95.98% | Test loss: 1.2180 | Test acc: 87.77%\n",
      "Epoch: 784 | Train loss: 0.1778 | Train acc: 95.59% | Test loss: 1.1447 | Test acc: 87.84%\n",
      "Epoch: 785 | Train loss: 0.1447 | Train acc: 96.14% | Test loss: 1.1562 | Test acc: 87.87%\n",
      "Epoch: 786 | Train loss: 0.1747 | Train acc: 95.70% | Test loss: 1.1175 | Test acc: 87.36%\n",
      "Epoch: 787 | Train loss: 0.1661 | Train acc: 95.80% | Test loss: 1.1828 | Test acc: 87.59%\n",
      "Epoch: 788 | Train loss: 0.1438 | Train acc: 96.21% | Test loss: 1.1800 | Test acc: 87.84%\n",
      "Epoch: 789 | Train loss: 0.1446 | Train acc: 96.21% | Test loss: 1.1335 | Test acc: 88.27%\n",
      "Epoch: 790 | Train loss: 0.1373 | Train acc: 96.39% | Test loss: 1.1780 | Test acc: 87.76%\n",
      "Epoch: 791 | Train loss: 0.1439 | Train acc: 96.27% | Test loss: 1.0790 | Test acc: 87.99%\n",
      "Epoch: 792 | Train loss: 0.1303 | Train acc: 96.45% | Test loss: 1.1900 | Test acc: 88.69%\n",
      "Epoch: 793 | Train loss: 0.1452 | Train acc: 96.31% | Test loss: 1.1676 | Test acc: 87.99%\n",
      "Epoch: 794 | Train loss: 0.1528 | Train acc: 96.16% | Test loss: 1.2567 | Test acc: 87.54%\n",
      "Epoch: 795 | Train loss: 0.1394 | Train acc: 96.27% | Test loss: 1.2420 | Test acc: 88.41%\n",
      "Epoch: 796 | Train loss: 0.1422 | Train acc: 96.32% | Test loss: 1.1045 | Test acc: 87.67%\n",
      "Epoch: 797 | Train loss: 0.1559 | Train acc: 96.06% | Test loss: 1.2078 | Test acc: 87.09%\n",
      "Epoch: 798 | Train loss: 0.1652 | Train acc: 95.86% | Test loss: 1.2230 | Test acc: 87.87%\n",
      "Epoch: 799 | Train loss: 0.1658 | Train acc: 95.75% | Test loss: 1.1549 | Test acc: 88.20%\n",
      "Epoch: 800 | Train loss: 0.1557 | Train acc: 96.04% | Test loss: 1.1672 | Test acc: 86.89%\n",
      "Epoch: 801 | Train loss: 0.1577 | Train acc: 95.88% | Test loss: 1.1245 | Test acc: 87.00%\n",
      "Epoch: 802 | Train loss: 0.1540 | Train acc: 96.18% | Test loss: 1.2752 | Test acc: 88.10%\n",
      "Epoch: 803 | Train loss: 0.1753 | Train acc: 95.69% | Test loss: 1.2167 | Test acc: 88.04%\n",
      "Epoch: 804 | Train loss: 0.1326 | Train acc: 96.40% | Test loss: 1.1806 | Test acc: 88.48%\n",
      "Epoch: 805 | Train loss: 0.1334 | Train acc: 96.42% | Test loss: 1.1315 | Test acc: 87.78%\n",
      "Epoch: 806 | Train loss: 0.1434 | Train acc: 96.32% | Test loss: 1.2776 | Test acc: 88.36%\n",
      "Epoch: 807 | Train loss: 0.1464 | Train acc: 96.19% | Test loss: 1.2112 | Test acc: 88.32%\n",
      "Epoch: 808 | Train loss: 0.1424 | Train acc: 96.28% | Test loss: 1.1744 | Test acc: 88.37%\n",
      "Epoch: 809 | Train loss: 0.1345 | Train acc: 96.37% | Test loss: 1.2315 | Test acc: 88.74%\n",
      "Epoch: 810 | Train loss: 0.1245 | Train acc: 96.71% | Test loss: 1.2854 | Test acc: 88.23%\n",
      "Epoch: 811 | Train loss: 0.1430 | Train acc: 96.31% | Test loss: 1.2554 | Test acc: 88.06%\n",
      "Epoch: 812 | Train loss: 0.1545 | Train acc: 96.23% | Test loss: 1.1628 | Test acc: 87.11%\n",
      "Epoch: 813 | Train loss: 0.1537 | Train acc: 96.00% | Test loss: 1.2696 | Test acc: 88.28%\n",
      "Epoch: 814 | Train loss: 0.1434 | Train acc: 96.19% | Test loss: 1.1570 | Test acc: 87.90%\n",
      "Epoch: 815 | Train loss: 0.1455 | Train acc: 96.30% | Test loss: 1.2357 | Test acc: 88.44%\n",
      "Epoch: 816 | Train loss: 0.1540 | Train acc: 96.12% | Test loss: 1.1000 | Test acc: 87.41%\n",
      "Epoch: 817 | Train loss: 0.1351 | Train acc: 96.39% | Test loss: 1.2831 | Test acc: 88.30%\n",
      "Epoch: 818 | Train loss: 0.1274 | Train acc: 96.64% | Test loss: 1.2470 | Test acc: 87.76%\n",
      "Epoch: 819 | Train loss: 0.1426 | Train acc: 96.35% | Test loss: 1.2655 | Test acc: 88.33%\n",
      "Epoch: 820 | Train loss: 0.1542 | Train acc: 96.07% | Test loss: 1.2349 | Test acc: 87.27%\n",
      "Epoch: 821 | Train loss: 0.1565 | Train acc: 96.09% | Test loss: 1.1822 | Test acc: 88.26%\n",
      "Epoch: 822 | Train loss: 0.1636 | Train acc: 95.93% | Test loss: 1.2886 | Test acc: 87.76%\n",
      "Epoch: 823 | Train loss: 0.1258 | Train acc: 96.59% | Test loss: 1.1190 | Test acc: 88.08%\n",
      "Epoch: 824 | Train loss: 0.1331 | Train acc: 96.41% | Test loss: 1.2027 | Test acc: 88.27%\n",
      "Epoch: 825 | Train loss: 0.1557 | Train acc: 96.17% | Test loss: 1.2101 | Test acc: 88.25%\n",
      "Epoch: 826 | Train loss: 0.1700 | Train acc: 95.76% | Test loss: 1.2432 | Test acc: 88.45%\n",
      "Epoch: 827 | Train loss: 0.1807 | Train acc: 95.64% | Test loss: 1.1388 | Test acc: 87.76%\n",
      "Epoch: 828 | Train loss: 0.1646 | Train acc: 95.78% | Test loss: 1.0310 | Test acc: 87.73%\n",
      "Epoch: 829 | Train loss: 0.1404 | Train acc: 96.31% | Test loss: 1.2124 | Test acc: 88.04%\n",
      "Epoch: 830 | Train loss: 0.1585 | Train acc: 95.98% | Test loss: 1.0813 | Test acc: 86.97%\n",
      "Epoch: 831 | Train loss: 0.1567 | Train acc: 96.05% | Test loss: 1.1029 | Test acc: 88.27%\n",
      "Epoch: 832 | Train loss: 0.1413 | Train acc: 96.27% | Test loss: 1.1788 | Test acc: 87.87%\n",
      "Epoch: 833 | Train loss: 0.1304 | Train acc: 96.64% | Test loss: 1.2143 | Test acc: 87.13%\n",
      "Epoch: 834 | Train loss: 0.1415 | Train acc: 96.30% | Test loss: 1.2203 | Test acc: 87.37%\n",
      "Epoch: 835 | Train loss: 0.1938 | Train acc: 95.44% | Test loss: 1.2980 | Test acc: 87.93%\n",
      "Epoch: 836 | Train loss: 0.1681 | Train acc: 95.81% | Test loss: 1.2194 | Test acc: 87.26%\n",
      "Epoch: 837 | Train loss: 0.1440 | Train acc: 96.11% | Test loss: 1.0954 | Test acc: 87.14%\n",
      "Epoch: 838 | Train loss: 0.1348 | Train acc: 96.50% | Test loss: 1.1239 | Test acc: 87.78%\n",
      "Epoch: 839 | Train loss: 0.1232 | Train acc: 96.69% | Test loss: 1.3547 | Test acc: 88.18%\n",
      "Epoch: 840 | Train loss: 0.1518 | Train acc: 96.22% | Test loss: 1.2735 | Test acc: 87.48%\n",
      "Epoch: 841 | Train loss: 0.1608 | Train acc: 96.00% | Test loss: 1.1437 | Test acc: 87.97%\n",
      "Epoch: 842 | Train loss: 0.1637 | Train acc: 95.87% | Test loss: 1.1974 | Test acc: 88.63%\n",
      "Epoch: 843 | Train loss: 0.1416 | Train acc: 96.22% | Test loss: 1.2087 | Test acc: 88.39%\n",
      "Epoch: 844 | Train loss: 0.1341 | Train acc: 96.52% | Test loss: 1.2064 | Test acc: 87.65%\n",
      "Epoch: 845 | Train loss: 0.1432 | Train acc: 96.28% | Test loss: 1.2178 | Test acc: 88.26%\n",
      "Epoch: 846 | Train loss: 0.1575 | Train acc: 96.03% | Test loss: 1.1502 | Test acc: 88.34%\n",
      "Epoch: 847 | Train loss: 0.1608 | Train acc: 96.05% | Test loss: 1.1522 | Test acc: 87.65%\n",
      "Epoch: 848 | Train loss: 0.1588 | Train acc: 95.92% | Test loss: 1.1755 | Test acc: 88.19%\n",
      "Epoch: 849 | Train loss: 0.1407 | Train acc: 96.33% | Test loss: 1.2530 | Test acc: 87.48%\n",
      "Epoch: 850 | Train loss: 0.1520 | Train acc: 96.04% | Test loss: 1.2873 | Test acc: 86.83%\n",
      "Epoch: 851 | Train loss: 0.1937 | Train acc: 95.37% | Test loss: 1.1770 | Test acc: 88.03%\n",
      "Epoch: 852 | Train loss: 0.1802 | Train acc: 95.62% | Test loss: 1.1287 | Test acc: 88.37%\n",
      "Epoch: 853 | Train loss: 0.1440 | Train acc: 96.25% | Test loss: 1.1882 | Test acc: 87.34%\n",
      "Epoch: 854 | Train loss: 0.1308 | Train acc: 96.46% | Test loss: 1.1332 | Test acc: 87.92%\n",
      "Epoch: 855 | Train loss: 0.1587 | Train acc: 95.94% | Test loss: 1.2242 | Test acc: 87.34%\n",
      "Epoch: 856 | Train loss: 0.1619 | Train acc: 95.94% | Test loss: 1.1297 | Test acc: 87.51%\n",
      "Epoch: 857 | Train loss: 0.1356 | Train acc: 96.31% | Test loss: 1.2316 | Test acc: 88.17%\n",
      "Epoch: 858 | Train loss: 0.1265 | Train acc: 96.62% | Test loss: 1.1485 | Test acc: 88.28%\n",
      "Epoch: 859 | Train loss: 0.1503 | Train acc: 96.16% | Test loss: 1.1080 | Test acc: 88.34%\n",
      "Epoch: 860 | Train loss: 0.1321 | Train acc: 96.49% | Test loss: 1.1713 | Test acc: 88.01%\n",
      "Epoch: 861 | Train loss: 0.1449 | Train acc: 96.25% | Test loss: 1.1335 | Test acc: 88.27%\n",
      "Epoch: 862 | Train loss: 0.1246 | Train acc: 96.61% | Test loss: 1.2696 | Test acc: 88.14%\n",
      "Epoch: 863 | Train loss: 0.1298 | Train acc: 96.50% | Test loss: 1.2836 | Test acc: 87.63%\n",
      "Epoch: 864 | Train loss: 0.1596 | Train acc: 96.08% | Test loss: 1.3096 | Test acc: 87.37%\n",
      "Epoch: 865 | Train loss: 0.1732 | Train acc: 95.81% | Test loss: 1.2408 | Test acc: 87.03%\n",
      "Epoch: 866 | Train loss: 0.1458 | Train acc: 96.13% | Test loss: 1.1702 | Test acc: 87.79%\n",
      "Epoch: 867 | Train loss: 0.1400 | Train acc: 96.34% | Test loss: 1.1486 | Test acc: 88.60%\n",
      "Epoch: 868 | Train loss: 0.1392 | Train acc: 96.34% | Test loss: 1.2411 | Test acc: 88.68%\n",
      "Epoch: 869 | Train loss: 0.1460 | Train acc: 96.30% | Test loss: 1.1436 | Test acc: 87.33%\n",
      "Epoch: 870 | Train loss: 0.1510 | Train acc: 96.13% | Test loss: 1.1149 | Test acc: 88.08%\n",
      "Epoch: 871 | Train loss: 0.1616 | Train acc: 96.03% | Test loss: 1.1355 | Test acc: 88.11%\n",
      "Epoch: 872 | Train loss: 0.1319 | Train acc: 96.52% | Test loss: 1.1692 | Test acc: 88.14%\n",
      "Epoch: 873 | Train loss: 0.1095 | Train acc: 96.96% | Test loss: 1.1967 | Test acc: 88.68%\n",
      "Epoch: 874 | Train loss: 0.1439 | Train acc: 96.31% | Test loss: 1.2084 | Test acc: 88.08%\n",
      "Epoch: 875 | Train loss: 0.1476 | Train acc: 96.16% | Test loss: 1.2363 | Test acc: 88.44%\n",
      "Epoch: 876 | Train loss: 0.1263 | Train acc: 96.62% | Test loss: 1.0865 | Test acc: 88.31%\n",
      "Epoch: 877 | Train loss: 0.1504 | Train acc: 96.35% | Test loss: 1.2267 | Test acc: 87.52%\n",
      "Epoch: 878 | Train loss: 0.1830 | Train acc: 95.54% | Test loss: 1.3944 | Test acc: 87.65%\n",
      "Epoch: 879 | Train loss: 0.1615 | Train acc: 95.89% | Test loss: 1.1623 | Test acc: 86.84%\n",
      "Epoch: 880 | Train loss: 0.1525 | Train acc: 96.18% | Test loss: 1.1698 | Test acc: 88.06%\n",
      "Epoch: 881 | Train loss: 0.1876 | Train acc: 95.53% | Test loss: 1.2568 | Test acc: 87.25%\n",
      "Epoch: 882 | Train loss: 0.1675 | Train acc: 95.89% | Test loss: 1.1585 | Test acc: 87.62%\n",
      "Epoch: 883 | Train loss: 0.1468 | Train acc: 96.22% | Test loss: 1.1997 | Test acc: 87.86%\n",
      "Epoch: 884 | Train loss: 0.1692 | Train acc: 95.81% | Test loss: 1.3672 | Test acc: 87.17%\n",
      "Epoch: 885 | Train loss: 0.1559 | Train acc: 96.02% | Test loss: 1.1022 | Test acc: 88.52%\n",
      "Epoch: 886 | Train loss: 0.1423 | Train acc: 96.31% | Test loss: 1.1790 | Test acc: 88.00%\n",
      "Epoch: 887 | Train loss: 0.1560 | Train acc: 96.09% | Test loss: 1.2071 | Test acc: 87.21%\n",
      "Epoch: 888 | Train loss: 0.1620 | Train acc: 95.93% | Test loss: 1.2000 | Test acc: 87.82%\n",
      "Epoch: 889 | Train loss: 0.1409 | Train acc: 96.37% | Test loss: 1.1531 | Test acc: 87.79%\n",
      "Epoch: 890 | Train loss: 0.1476 | Train acc: 96.28% | Test loss: 1.2201 | Test acc: 87.67%\n",
      "Epoch: 891 | Train loss: 0.1356 | Train acc: 96.46% | Test loss: 1.1718 | Test acc: 87.36%\n",
      "Epoch: 892 | Train loss: 0.1630 | Train acc: 96.02% | Test loss: 1.2360 | Test acc: 88.11%\n",
      "Epoch: 893 | Train loss: 0.1552 | Train acc: 96.06% | Test loss: 1.1998 | Test acc: 88.37%\n",
      "Epoch: 894 | Train loss: 0.1526 | Train acc: 96.21% | Test loss: 1.1819 | Test acc: 88.18%\n",
      "Epoch: 895 | Train loss: 0.1506 | Train acc: 96.21% | Test loss: 1.2173 | Test acc: 88.43%\n",
      "Epoch: 896 | Train loss: 0.1404 | Train acc: 96.44% | Test loss: 1.2103 | Test acc: 87.51%\n",
      "Epoch: 897 | Train loss: 0.1488 | Train acc: 96.17% | Test loss: 1.0995 | Test acc: 87.57%\n",
      "Epoch: 898 | Train loss: 0.1436 | Train acc: 96.28% | Test loss: 1.2345 | Test acc: 87.69%\n",
      "Epoch: 899 | Train loss: 0.1617 | Train acc: 96.01% | Test loss: 1.3303 | Test acc: 88.20%\n",
      "Epoch: 900 | Train loss: 0.1783 | Train acc: 95.86% | Test loss: 1.1758 | Test acc: 87.66%\n",
      "Epoch: 901 | Train loss: 0.1569 | Train acc: 95.99% | Test loss: 1.3233 | Test acc: 87.68%\n",
      "Epoch: 902 | Train loss: 0.1643 | Train acc: 96.02% | Test loss: 1.1895 | Test acc: 87.44%\n",
      "Epoch: 903 | Train loss: 0.1598 | Train acc: 96.00% | Test loss: 1.3473 | Test acc: 87.75%\n",
      "Epoch: 904 | Train loss: 0.1545 | Train acc: 96.14% | Test loss: 1.2682 | Test acc: 88.28%\n",
      "Epoch: 905 | Train loss: 0.1524 | Train acc: 96.12% | Test loss: 1.1834 | Test acc: 88.94%\n",
      "Epoch: 906 | Train loss: 0.1309 | Train acc: 96.47% | Test loss: 1.2060 | Test acc: 87.94%\n",
      "Epoch: 907 | Train loss: 0.1559 | Train acc: 96.19% | Test loss: 1.1087 | Test acc: 88.16%\n",
      "Epoch: 908 | Train loss: 0.1442 | Train acc: 96.30% | Test loss: 1.2039 | Test acc: 88.01%\n",
      "Epoch: 909 | Train loss: 0.1245 | Train acc: 96.66% | Test loss: 1.2408 | Test acc: 88.31%\n",
      "Epoch: 910 | Train loss: 0.1430 | Train acc: 96.33% | Test loss: 1.3285 | Test acc: 88.24%\n",
      "Epoch: 911 | Train loss: 0.1918 | Train acc: 95.55% | Test loss: 1.2820 | Test acc: 86.38%\n",
      "Epoch: 912 | Train loss: 0.1607 | Train acc: 95.89% | Test loss: 1.3044 | Test acc: 87.57%\n",
      "Epoch: 913 | Train loss: 0.1800 | Train acc: 95.58% | Test loss: 1.1991 | Test acc: 88.56%\n",
      "Epoch: 914 | Train loss: 0.1576 | Train acc: 95.97% | Test loss: 1.3549 | Test acc: 87.37%\n",
      "Epoch: 915 | Train loss: 0.1657 | Train acc: 95.94% | Test loss: 1.2277 | Test acc: 87.79%\n",
      "Epoch: 916 | Train loss: 0.1609 | Train acc: 96.01% | Test loss: 1.1564 | Test acc: 88.17%\n",
      "Epoch: 917 | Train loss: 0.1487 | Train acc: 96.19% | Test loss: 1.1102 | Test acc: 88.43%\n",
      "Epoch: 918 | Train loss: 0.1523 | Train acc: 96.21% | Test loss: 1.3337 | Test acc: 87.65%\n",
      "Epoch: 919 | Train loss: 0.1833 | Train acc: 95.61% | Test loss: 1.1027 | Test acc: 88.10%\n",
      "Epoch: 920 | Train loss: 0.1726 | Train acc: 95.75% | Test loss: 1.2314 | Test acc: 87.63%\n",
      "Epoch: 921 | Train loss: 0.1514 | Train acc: 96.01% | Test loss: 1.1929 | Test acc: 87.19%\n",
      "Epoch: 922 | Train loss: 0.1432 | Train acc: 96.27% | Test loss: 1.1779 | Test acc: 88.39%\n",
      "Epoch: 923 | Train loss: 0.1409 | Train acc: 96.36% | Test loss: 1.1776 | Test acc: 88.68%\n",
      "Epoch: 924 | Train loss: 0.1324 | Train acc: 96.48% | Test loss: 1.0992 | Test acc: 87.17%\n",
      "Epoch: 925 | Train loss: 0.1522 | Train acc: 96.19% | Test loss: 1.1553 | Test acc: 88.47%\n",
      "Epoch: 926 | Train loss: 0.1543 | Train acc: 96.17% | Test loss: 1.0957 | Test acc: 87.10%\n",
      "Epoch: 927 | Train loss: 0.1409 | Train acc: 96.39% | Test loss: 1.2459 | Test acc: 87.78%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results\u001b[38;5;241m=\u001b[39mtrain(model\u001b[38;5;241m=\u001b[39mmodel,train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,test_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,optimizer\u001b[38;5;241m=\u001b[39moptimizer,device\u001b[38;5;241m=\u001b[39mdevice,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, loss_fn, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 57\u001b[0m     train_loss,train_acc\u001b[38;5;241m=\u001b[39mtrain_step(model\u001b[38;5;241m=\u001b[39mmodel,data_loader\u001b[38;5;241m=\u001b[39mtrain_dataloader,loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,optimizer\u001b[38;5;241m=\u001b[39moptimizer,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m     test_loss,test_acc\u001b[38;5;241m=\u001b[39mtest_step(model\u001b[38;5;241m=\u001b[39mmodel,data_loader\u001b[38;5;241m=\u001b[39mtest_dataloader,loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% | Test loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, data_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m train_loss,train_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch,(X,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m     X,y\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mto(device),y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_course/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_course/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_course/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_course/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_course/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_course/lib/python3.12/site-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "results=train(model=model,train_dataloader=train_dataloader,test_dataloader=test_dataloader,loss_fn=loss_fn,optimizer=optimizer,device=device,epochs=1000)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
